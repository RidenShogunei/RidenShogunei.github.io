<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models 论文阅读笔记</title>
    <link href="/2024/12/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/12/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>其实和TTT没啥关系，但是还是看，反正我时间很多（链接：<a href="https://arxiv.org/pdf/2411.19477%EF%BC%89">https://arxiv.org/pdf/2411.19477）</a></p><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><h2 id="个人初步总结"><a href="#个人初步总结" class="headerlink" title="个人初步总结"></a>个人初步总结</h2><p>这篇论文实际上就是提出了一套流程，在没有提出新的模型架构的情况下用新的计算流程让整个系统准确率能更高，但是有一定的要求（解决问题的LLM一定要能给出正确的答案，如果给出正确答案的概率是0也白搭），然后作者从数学原理上证明了他这套系统在一定情况下能把错误率减到0。<br>这个就好比什么呢，LLM是一个纯黑盒的回答机器，他会根据问题输出答案，但是得出正确答案的概率会变，可能从5%到90%都有可能，所以一般人都不会真正去相信它，因为没有办法让这个玩意给出正确答案的概率升级到99%，这样大家就会相信它了。<em><strong>所以作者就给出了一套系统，它让我们在完全不改动这个LLM黑盒本身的回答正确概率的情况下，让这个新的系统用LLM黑盒得出正确率能到99%甚至100%的回答，这样就让大家能完全的相信这玩意。</strong></em><br>我看完就说，感觉是在弓箭时代没去研究改变弓箭的结构让箭得更准，而是发明了弓箭手方阵这样总有射中的箭，但是凡事都有代价，这个系统的代价肯定是会在推理阶段花更多的力气。</p><h2 id="作者的系统"><a href="#作者的系统" class="headerlink" title="作者的系统"></a>作者的系统</h2><p>有一说一，作者的系统其实非常好懂，就是大力出奇迹，生成一堆答案，然后通过一个系统来筛选出一个正确率最高的答案，不过我还是认真写一下流程吧，数学分析也会做的（但是会在下一个小节<br>本质上就是两步：</p><h3 id="N个答案的生成"><a href="#N个答案的生成" class="headerlink" title="N个答案的生成"></a>N个答案的生成</h3><p>用多个LLM来生成，很简单吧，但是这篇文章的作者是用70B和72B的模型做的。。。阿里真是财大气粗啊</p><h3 id="淘汰赛"><a href="#淘汰赛" class="headerlink" title="淘汰赛"></a>淘汰赛</h3><p>这个是重点，怎么在这么多的回答中筛出正确的答案呢</p><blockquote><p>for  K times. The winner of each pair is the one that is favored for more than K&#x2F;2 times; ties are broken arbitrarily. Only the winners will move on to the next round. The ﬁnal-round winner at the end of this tournament will be the ﬁnal output of the algorithm.</p></blockquote><p><img src="https://i-blog.csdnimg.cn/img_convert/143b9ab562e90bb65e41af91e45fa955.png#pic_center" alt="淘汰赛示意图"></p><p>上面是原文和图片，下面用我的话讲一遍：<br><strong>目标</strong>： 从多个候选答案中选出最佳答案作为最终输出。<br><strong>过程</strong>：</p><ol><li><strong>随机配对</strong>： 将候选答案随机分成一对一对的。</li><li><strong>多次比较</strong>： 对每一对候选答案进行比较 K 次。每次比较都会选择一个胜者。</li><li><strong>选择胜者</strong>： 比较结束后，获得超过 K&#x2F;2 次胜利的候选答案被视为该轮的胜者。如果出现平局，则随机选择一个胜者。</li><li><strong>下一轮</strong>： 只有胜者才能进入下一轮比赛。</li><li><strong>最终结果</strong>： 经过多轮比赛，最终剩下的胜者即为算法的最终输出。<br><strong>举例说明</strong>：<br>假设有 8 个候选答案，K &#x3D; 2。</li></ol><ul><li><strong>第一轮</strong>： 将 8 个候选答案随机配对成 4 组，每组进行比较 2 次。例如，假设第一组两个候选答案的比较结果分别为 A 胜 B 和 B 胜 A，那么这一组的胜者为 B。</li><li><strong>第二轮</strong>： 将 4 个胜者再次随机配对成 2 组，每组进行比较 2 次。例如，假设第一组的胜者为 B 和 D，比较结果分别为 B 胜 D 和 D 胜 B，那么这一组的胜者为 D。</li><li><strong>第三轮</strong>： 最后 2 个胜者进行比较 2 次，例如，假设胜者为 D 和 E，比较结果分别为 D 胜 E 和 E 胜 D，那么最终的胜者为 D，即算法的最终输出。<br><strong>这个淘汰赛的过程类似于体育比赛中的淘汰赛制，通过不断筛选，最终选出最佳的候选答案</strong>。<br>那么比较这个动作是谁来做呢？显然比较这个动作是由 LLM 来完成的。LLM 被用作一个黑盒模型，负责评估两个候选答案的优劣，并输出比较结果。</li></ul><p>上面其实就是所有流程了，你可能注意到了，这个玩意需要一些限制才能保证最后的正确率能达到很高，这个也是作者后面的数学推理的基础：</p><ol><li>LLM给出答案的正确率不能是0，不然你再怎么比较也不可能保证这个系统的正确率能到100%，如果有一次给出的都是错误答案那你再选也没辙。</li><li>LLM做比较的正确率大于你随机选择（这个动作的正确率是50%）</li></ol><p>在这两个假设的限制下，这个系统的成功率才能到100%（理论上）</p><h2 id="数学分析部分"><a href="#数学分析部分" class="headerlink" title="数学分析部分"></a>数学分析部分</h2><p>数学分析部分其实也很好理解,没有涉及很多很难的东西.<br>首先先介绍一下后面要用的变量<br>N和K,分别代表着生成N个答案和进行K次比较,然后是<br><img src="https://i-blog.csdnimg.cn/img_convert/8480aba7c52a944cd3d3f1bda8ae9470.png"><br>在这段描述中：</p><ul><li><p><strong>$M_{gen}$</strong> 代表的是在一次大型语言模型（LLM）调用以生成候选解决方案时，其输出结果的概率分布。也就是说，当利用LLM去尝试生成一个有可能解决给定问题的候选方案时，这个生成过程对应的输出所遵循的概率分布情况由 $M_{gen}$ 来表示。</p></li><li><p><strong>$M_{comp}$</strong> 代表的是在利用LLM对一对解决方案进行比较时，其输出结果的概率分布。例如针对两个已经存在的解决方案，使用LLM来判断它们谁更优或者它们之间的相对关系等情况时，LLM此次比较操作输出所遵循的概率分布就用 $M_{comp}$ 来表示。</p></li></ul><p>所以给定一个输入问题X,在算法的第一个阶段,答案是符合 $M_{gen}$ 的分布的:<br><img src="https://i-blog.csdnimg.cn/img_convert/6adfb7bc78552e938682cf1ca050eeee.png"><br>然后在第二阶段,淘汰赛阶段,对于每一个候选者$(y,y^{\prime})$,算法对于$K$个独立比较结果进行采样:<br><img src="https://i-blog.csdnimg.cn/img_convert/2c1f149a29f41a14cfc08f1b742c3fd7.png"><br>这是基本的流程,然后接下来是两条定理,限制了计算的情况:<br> 1.<br> 第一条定律,模型输出正确答案的概率不能为0,必然要大于0<img src="https://i-blog.csdnimg.cn/img_convert/9e861de01d6075961946e7785e25cf1d.png"><br> 2.<br> 第二条定律,比较一对正确和错误的解决方案时比随机猜测做得更好也就是说政正确概率大于50%<img src="https://i-blog.csdnimg.cn/img_convert/7937f2ae4549d39d12222e1eedac433b.png"></p><p> 在这两条规律的限制下开始推导:</p><ul><li>定理一:如果输入问题满足假设 1，则所提出的两阶段算法在超参数 N和 K下的失败概率有如下界限：<img src="https://i-blog.csdnimg.cn/img_convert/f9dfd445ae8c6d773cc116c9f0474036.png"><br> 看起来很复杂的公式,不是么,接下来开始推导:</li></ul><ol><li>第一阶段,也就是生成部分,独立的进行了N次采样,所以我们的无正确解的概率是:<img src="https://i-blog.csdnimg.cn/img_convert/b19acb338377ef2c631ea1d80ee6c0b7.png">显然</li><li>然后是淘汰赛阶段,这个会稍微复杂一点,先考虑一对正确和错误的候选方案,他们被比较了K次,而且每次比较的结果的正确率—下面的公式称之为µ—-都大于随机选择(也就是说大于50%),Xi表示均值为µ的独立伯努利随机变量：所以我们得出,经历了K次比较之后失败的概率公式:<img src="https://i-blog.csdnimg.cn/img_convert/f0dd720fde5211aafe156d9394d558c6.png"><img src="https://i-blog.csdnimg.cn/img_convert/3c634bc316251e562557f6107099ff02.png">这里我就先不显然了,因为这里是做了一定的推理的:</li></ol><ul><li>首先对于最早的不等式我做一个解释,Xi的本质含义就是第i个比较是否正确,所以对于Xi的求和的本质就是看K次比较里正确的总次数,显然比较失败意味着正确识别的次数少于一半,也就是:<img src="https://i-blog.csdnimg.cn/img_convert/e03d28678d3a75f9c82f6bb177943a6b.png">所以失败的概率实际上会等于上面这个式子,但是作者用了一个放缩先写成了小于等于,再进一步的写法就是:<img src="https://i-blog.csdnimg.cn/img_convert/038b94048c62a24a71ecafb44aa10e19.png">当平均次数小于等于0.5时，就认为比较失败了(符合第二个假设)。</li><li>然后在此引入均值μ,这一步就是不等式两边减了一个μ,为了后面的不等式做准备</li><li>使用 Hoeffding 不等式:<img src="https://i-blog.csdnimg.cn/img_convert/eced9703521333d7d831e455b7cb115f.png">,所以直接套公式就好,然后又因为已知μ&gt;Pcomp&gt;0.5,所以又能进行一次替换,换成最后的形式,最终完成了对比较这对候选解决方案失败概率的推导和界定。</li></ul><ol start="3"><li>最后就是把两个阶段的概率合在一起来看:在控制淘汰赛阶段失败概率时，先基于生成阶段至少有一个正确初始候选方案这一前提。任选一个正确候选方案，关注其在二叉树结构里通向算法最终输出的路径。通过归纳法可证明，路径上每对候选方案用(K)次LLM调用比较时，若有一个输入候选方案正确，其比较失败概率不超<img src="https://i-blog.csdnimg.cn/img_convert/5f0edb64fd85fa9f3352eff4016dce78.png">沿此含log2N对比较的路径，用并集界可知整体比较得出正确结果（即算法最终输出正确）的失败概率不超<img src="https://i-blog.csdnimg.cn/img_convert/54b2d651adf00ab5b35611b308400584.png">，以此对算法在该阶段输出正确结果的概率情况进行了分析界定。 最后，对算法两个阶段的失败事件取并集，完成了对定理 1 的证明。</li></ol><p><em><strong>所以说失败概率随着确定测试时间计算量的超参数 N和 K指数衰减至零。</strong></em></p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>看完的感觉就是：</p><blockquote><p>大力出奇迹啊，作者用的还是72B的和70B的模型做的实验（用了阿里自己的通义千问，他这套系统还得好几个LLM（最多的实验有32LLM个并行）并行执行，阿里是真有钱啊<br>但是这种系统，虽然说没看过完全一样的，但是类似思想看过很多，比如说让LLM对一个问题生成多个答案然后通过投票系统啥的筛出最有可能的答案，这个和作者的并行生成多个回答的区别就在于没他快吧（，后面的筛选系统感觉有很多方法（作者这个用LLM来负责比较的系统还得加个LLM选择的正确概率大于随机选择的正确概率（50%）的假设<br>所以作者的创新点在于数学理论论证和并行生成并行筛选系统？而且他这个实验结果想要复现的话估计只有超级大企业能做吧（</p></blockquote><p>感觉大模型还是得跟企业一起搞啊，不然这么猛的算力高校哪里提供的了啊（</p><p>不过在我和老师交流过后,老师的回应也我也记录一下:</p><blockquote><p>这篇文章的定位其实是和大模型训练的 scaling law 一样。大模型的 scaling law 给大模型的训练提供了一个理论（其实是经验公式）指导，就是如何有效调配算法、数据、模型，比如要想提升模型性能，光增大模型 size 还不行，还需要提升数据集大小。<br>这篇文章是类似的，但关注点不在大模型训练上，而是test-time computation，简单来说就是模型参数不变，而是在模型输出&#x2F;解码过程中做文章。这里就有很多方法，但是大部分都是基于多次输出选最好的，可以是 best-of-N，可以是 voting，可以是更复杂的搜索，甚至可以训练一个小模型来指导大模型的输出&#x2F;解码过程（比如 OpenAI-o1，DeepSeek-o1，QwQ 等）。那么这里就有一个问题，是否在test-time computation 中投入更大的算力（比如并行数更大，搜索深度更深，小模型训练得更好）就可以带来大模型效果的提升呢？我们也需要一个 scaling law 来指导。这就是这篇文章想做的事情，通过大规模实验，来看看是否存在 test-time computation的 scaling law。</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>TTT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 论文阅读报告</title>
    <link href="/2024/12/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A-1/"/>
    <url>/2024/12/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A-1/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>因为最近都在看TTT相关的文章，所以继续看，这篇的来源是之前看的论文的（放一下链接：<a href="https://proceedings.mlr.press/v119/sun20b/sun20b.pdf">https://proceedings.mlr.press/v119/sun20b/sun20b.pdf</a> ）</p><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p>刚看完开头，没想到是在CV领域的一篇论文，果然这种结构其实都是可以通用的吗，虽然和llm没关系但是还是继续看，反正我时间很多（大概</p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>作者是怎么想到这个办法的？为什么会想到这个办法，或许这个问题有时候连作者本人都回答不了，但是我还是会很好奇的揣测一下。我在下面尝试推测一下：</p><ol><li>从绪论部分看的话，作者也是想要解决模型的泛化不强问题<blockquote><p>Supervised learning remains notoriously weak at generalization under distribution shifts. Unless training and test data are drawn from the same distribution, even seemingly minor differences turn out to defeat state-of-the-art models (Recht et al., 2018).</p></blockquote></li></ol><p>（ 真是一个永恒的问题），但是从理论上来说，一个模型的泛化能力再强，碰到训练集里从未出现的玩意之后还是难办，所以作者没有继续跟着前人的步伐，试图通过拓扑结构或训练期间可用的测试分布数据来预测训练和测试分布之间的差异，而是另辟蹊径，想到了直接在测试的时候，根据新的数据来更新参数，<br> 2. 但是问题又来了，<strong>这个时候更新，没有标签怎么办？</strong> 作者就用了自监督训练的办法来解决这个问题，所以接下来要找一个办法进行快速的自监督学习。<br> 3. 然后作者找到了一个很简单的办法（将每个输入图像旋转 90 度的倍数并预测其角度的任务 (Gidaris et al., 2018)。）用这个办法能实现自监督训练。如果多个测试样本以批次到达，可以使用整个批次进行测试时训练。如果样本以在线流的形式到达，可以通过保持参数状态来获得进一步的改进。（显然，这两种数据的数据方式也让作者在之后的流程中开发出了两种TTT办法）<br> 4. 但是如果要全参数重新训练调整的话，是不是效率太低了，可以不可以用一个辅助模型，只更新部分参数就好？这样的话，可以极大的提升效率（我能想到例子就是大模型的lora网络）（这个部分是我猜的）<br> 5. 好像整体的思路都跑通了，那具体怎么来操作呢？那就是我下面章节要说的。</p><h2 id="具体流程"><a href="#具体流程" class="headerlink" title="具体流程"></a>具体流程</h2><p>其实我不打算讲太多，只想提几个我认为重要的点，毕竟这篇文章主要还是给我自己看的（<br>就像上面我说的，作者的思路其实很清晰，但是每一步的实现会有一些细节的操作要提一下：</p><ol><li>自监督学习部分其实没有啥要提的，我感觉这个方法真挺有意思的：<blockquote><p>In this paper, we choose the rotation prediction task (Gidaris et al., 2018), which has been demonstrated to be simple and effective at feature learning for convolutional neural networks. The task simply rotates<br>x in the image plane by one of 0, 90, 180 and 270 degrees and have the model predict the angle of rotation as a fourway classiﬁcation problem.</p></blockquote></li></ol><p>可以说符合我对于简单但是好用办法的想象</p><ol start="2"><li>然后是训练部分，这里其实要提一点：作者提到了联合训练和预测时训练，<br> <strong>这里面联合训练 (Joint Training) 是指在训练阶段，主模型和辅助模型同时进行训练，共享部分参数。</strong><br>  <strong>而预测时训练 (Test-Time Training) 则是指在预测阶段，利用单个未标记测试样本进行自监督学习，更新共享的特征提取器参数，然后再进行预测。</strong></li></ol><ul><li>两者之间的关系：<br>  联合训练为预测时训练提供了基础，使其能够利用共享的特征提取器进行快     速学习。<br>  预测时训练则进一步提高了模型在分布偏移情况下的泛化能力，使其能够适应不同的测试分布。</li><li>总结：<br>联合训练和预测时训练是 Test-Time Training 方法的两个重要组成部分，它们相互配合，共同提高了模型的泛化能力。</li></ul><ol start="3"><li>然后是作者还提出了两种TTT模式：TTT 和 TTT-Online ，是两种训练方式</li></ol><ul><li>TTT (Test-Time Training)：标准版本，对于每个测试样本，使用辅助模型对其进行自监督学习，更新共享的特征提取器参数，然后使用更新后的参数进行预测。</li><li>TTT-Online (Online Test-Time Training)：在线版本，对于每个新的测试样本，使用上一次更新后的参数作为初始参数，进行自监督学习，更新共享的特征提取器参数，然后使用更新后的参数进行预测。这种模式允许模型在测试阶段持续学习，并根据新的测试样本不断调整其参数。<br><em><strong>两种模式的区别：</strong></em></li><li>TTT： 每个测试样本都会独立地进行自监督学习和预测，参数更新后的结果不会影响后续测试样本。</li><li>TTT-Online： 每个测试样本的参数更新都会基于上一次更新后的参数，这意味着模型会持续学习并积累经验，从而更好地适应分布偏移。<br><em><strong>适用场景：</strong></em></li><li>TTT： 适用于测试样本是独立同分布的情况，例如在标准的图像分类任务中。</li><li>TTT-Online： 适用于测试样本不是独立同分布的情况，例如在视频帧识别任务中，后续帧可能受到先前帧的影响。</li></ul><h2 id="我学到了什么"><a href="#我学到了什么" class="headerlink" title="我学到了什么"></a>我学到了什么</h2><p>看这篇论文非常快，作者逻辑很清晰，公式很好理解，方法很明晰，当然也可能是因为结果部分占比比较多而我又跳着看这个（因为结果的各类比较对我来说没有什么作用</p><p>总之看完之后，我感觉我的收获就是对TTT理解更多了一些，TTT本质就是在测试阶段想要使用自监督学习来进行部分参数的调整来应对不在训练集的问题，所以在llm中，用的就是lora来当作辅助模型这个部分来更新参数，我感觉这个操作现在在我看来是一个更加顺理成章的操作了。</p><p>同时看完这篇文章之后我还更加理解了一个问题，就是辅助网络要经历两次训练，一次是跟着主模型训练，一次是推测时训练，也就是上面提到的联合训练和预测时训练，这样操作的话不仅是提升了辅助网络的泛化能力，<strong>还因为辅助模型会生成自监督任务</strong>，所以能够为主模型提供额外的监督信号</p><blockquote><p><strong>因为在联合训练中，使用相同的训练数据同时训练主模型和辅助模型。在训练过程中，主模型的损失函数和辅助模型的损失函数会进行加权求和，并用于计算所有参数的梯度</strong></p></blockquote><p>这有助于防止主模型在训练数据上过拟合，从而提高模型在测试数据上的性能。<br><em><strong>但是需要注意，在联合训练过程中，两个模型的训练任务时不一样的，主任务是图像分类，辅助任务是图像旋转预测，这是辅助模型能为主模型提供额外的监督信号的原因。</strong></em></p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>这次的文章结构又变了，我感觉还是会继续变，随着我看的论文越来越多，我记录的东西的深度和广度一定是会变化的，我想记录的，对我来说有用的东西肯定也是会变化的，这很正常。</p>]]></content>
    
    
    
    <tags>
      
      <tag>TTT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>COMBINING INDUCTION AND TRANSDUCTION FOR ABSTRACT REASONING 论文阅读报告</title>
    <link href="/2024/12/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/"/>
    <url>/2024/12/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这是我最近阅读的第二篇TTT相关的论文（论文链接：<a href="https://arxiv.org/abs/2411.02272">https://arxiv.org/abs/2411.02272</a> ），这篇论文提到的方法在上一篇论文里也用到过，所以读一遍，但是有一点不同，我现在认为从头到尾注解式的讲一遍没有自己的思考，不是真正的阅读，所以我这一篇的行文格式会不太一样。我会先自己把论文认真看一遍，然后把自己的想法按照自己的思路写一遍，这样或许会更有收获？尽管这样的话，行文的顺序就和论文不太一样了，总是先试试吧。</p><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><h2 id="看完一遍的初步想法"><a href="#看完一遍的初步想法" class="headerlink" title="看完一遍的初步想法"></a>看完一遍的初步想法</h2><p>首先看标题的时候应该就知道本文主要的点在于研究inductive and transductive这两个方法的优劣和结合，看摘要发现果然，因为它一直在讨论究竟是inductive好呢还是transductive好呢，但是最后，果不其然，还是两个都有比较好。<br>所以关注的重点显然就在于经典的，是什么，为什么，怎么办</p><h3 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h3><p>在介绍部分实际上作者还是比较详细的讲了为啥有这么一个思路—-因为人类可以从少量数据中泛化推理出很多东西，但是机器显然不行，这种数据量少的学习都可以称之为Few-shot learning（这个在后面也会继续说），然后当然还要介绍一下两个重要的概念，<strong>归纳和转导</strong>，在这里引用作者的一段话 ：</p><blockquote><p>Here we study neural methods for induction and transduction, using few-shot learning problems from ARC-AGI as our testbed. Induction means ﬁrst ﬁnding a function f where f (xtrain) ≈ ytrain<br>, and then predicting ytest &#x3D; f (xtest). Transduction instead outputs ytest without explicit construction of an intermediate function f . Intuitively, induction captures the notion that a learner should ﬁrst explain the training data, then use that explanation to make predictions. Inductive learners can perform better by spending more time optimizing or searching for better explanations, using the training examples xtrain, ytrain to score candidate functions. Transduction instead captures the intuition that the training examples themselves should play a direct role in generating new predictions, and that successful prediction need not require an explicit explanation.</p></blockquote><p>这是在说啥呢，直观地说，归纳体现了学习者应该首先解释训练数据，然后使用该解释进行预测的概念。归纳学习者可以通过花费更多时间优化或搜索更好的解释来提高性能，使用训练示例<br>xtrain, ytrain 来对候选函数进行评分。转导则体现了训练示例本身应该在生成新预测中发挥直接作用，并且成功的预测不需要明确的解释。<br>所以这两种形式看起来是不太相关的，但是作者通过生成大量的合成问题来训练用于归纳和转导的神经网络之后发现了，用于归纳和转导的神经模型具有很强的互补性。这是比较令人惊讶的点，也是作者的创新点：<br><em><strong>在实际应用中，可以先使用归纳学习获取一个通用函数，如果模型的效果不理想或时间限制使然，可以启用转导学习作为回退机制，以增强预测能力。这种结合方法既能利用归纳学习的泛化能力，也能通过转导学习实现对特定问题的优化，从而在实践中达到更好的效果。</strong></em></p><h3 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h3><p>感觉我上面其实已经提到这个部分了，作者有这个创新点的原因主要是发现了归纳和转导模型之间的互补性，而且还有一点：虽然任何一对模型通常都会解决一些不同的问题，但通常这可以归因于不同的先验知识、数据或架构。但是作者还发现了，即使控制了先验知识、数据和架构，大多数通过归纳解决的问题都没有通过转导解决，反之亦然。所以这两类模型能解决的问题之前确实有很明显的互补性。正应如此，作者才会想办法结合两类模型，来达到1+1&gt;2 的操作</p><h3 id="怎么办"><a href="#怎么办" class="headerlink" title="怎么办"></a>怎么办</h3><p>就像我上面已经提到的，作者的创新点就在于结合了转导和归纳模型的优点，但是作者实现结合的办法并不是改变模型的结构，而是改变推理的流程（先转导不行就归纳），同时提出了一个有效的数据生成管线达到的，这也是作者提到的自己的贡献所在：</p><blockquote><p>An automated data generation methodology that starts with 100-160 program solutions for ARC training tasks, and expands them to make 400k new problems paired with Python solutions.</p></blockquote><p>那么具体来说怎么操作呢？</p><h4 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h4><h5 id="通过元学习来训练"><a href="#通过元学习来训练" class="headerlink" title="通过元学习来训练"></a>通过元学习来训练</h5><p>在训练归纳和转导模型时，采用了元学习（meta-learning）的策略。元学习本质上是一种学习如何学习的办法，即通过从大量的小样本学习任务中提取经验，以提升在新任务上的学习效率。以下是对如何使用元学习来训练两个类型模型的解释：</p><h6 id="元学习数据集"><a href="#元学习数据集" class="headerlink" title="元学习数据集"></a>元学习数据集</h6><p>假设有一个元学习数据集，它由许多小样本学习任务构成。每个任务都伴随着一个真实的函数 f，确保对于给定的训练数据和测试数据 ，都满足 f(x) &#x3D; y 。</p><p>这样的数据集提供了一种结构化的环境，使得模型可以从许多不同但相关的任务中获取经验。有了这个数据集后，模型可以在不同问题中练习，以提高在新任务上迅速适应的能力。</p><h6 id="归纳模型的训练"><a href="#归纳模型的训练" class="headerlink" title="归纳模型的训练"></a>归纳模型的训练</h6><p>归纳模型通过学习函数映射来进行推理。它们的训练目标是最小化一种损失函数，该损失函数通常基于预测的函数与实际的函数之间的差异。假设我们有一个对模型预测的函数分布的评估方法，那么归纳学习模型会在元学习数据集上通过调整参数  来最小化这种差异，以便学到一个适用于新数据的通用函数。</p><h6 id="转导模型的训练"><a href="#转导模型的训练" class="headerlink" title="转导模型的训练"></a>转导模型的训练</h6><p>转导模型直接优化特定测试样本的预测准确性。因此，其训练的损失函数会基于模型在这些具体样本上的预测误差来定义。通过在元学习数据集上进行训练，转导模型调整参数，以在给定的特定训练和测试样本条件下改进效果。</p><h5 id="测试归纳和转导模型"><a href="#测试归纳和转导模型" class="headerlink" title="测试归纳和转导模型"></a>测试归纳和转导模型</h5><p>测试归纳和转导。在元学习之后，模型会遇到一个测试时的小样本学习任务(xtrain, ytrain, xtest)。转导模型预测其对ytest通过束搜索近似）最可能的输出。归纳模型从B个函数f1 · · · fB的测试时预算中进行采样，这些函数通过(xtrain, ytrain)进行过滤，最后用于预测ytest &#x3D; f (xtest)。将预测的测试输出记为yˆtest</p><h5 id="归纳和转导的结合"><a href="#归纳和转导的结合" class="headerlink" title="归纳和转导的结合"></a>归纳和转导的结合</h5><p>结合归纳和转导。归纳允许将候选假设与训练样本进行比较，所以我们知道何时归纳找到了一个合理的解决方案，但有时它无法找到任何解决方案。转导具有相反的特性：我们无法检查其预测是否与训练样本匹配，但它总是提供一个候选答案。因此，我们首先尝试归纳，然后在没有候选假设能够解释样本的情况下尝试转导，这就是所谓的回退。</p><h4 id="数据生成方法（重要）"><a href="#数据生成方法（重要）" class="headerlink" title="数据生成方法（重要）"></a>数据生成方法（重要）</h4><h5 id="步骤-1：人工编写种子程序"><a href="#步骤-1：人工编写种子程序" class="headerlink" title="步骤 1：人工编写种子程序"></a>步骤 1：人工编写种子程序</h5><ul><li>人工编写少量解决特定任务的 Python 程序（据作者说是100个），这些程序将作为数据生成的起点。</li><li>每个种子程序包含：<ul><li><strong>自然语言描述</strong>： 解释该程序解决的任务类型和目标。</li><li><strong>Python 函数 transform_grid</strong>： 将输入网格转换为输出网格。</li><li><strong>Python 函数 generate_input</strong>： 随机生成输入网格。</li><li><strong>Python 库</strong>： 提供通用的代码功能，例如生成随机图形、检测对称性、提取物体等。</li></ul></li></ul><h5 id="步骤-2：LLM-生成新的语言描述"><a href="#步骤-2：LLM-生成新的语言描述" class="headerlink" title="步骤 2：LLM 生成新的语言描述"></a>步骤 2：LLM 生成新的语言描述</h5><ul><li>使用 LLM，根据种子程序的描述生成新的自然语言描述。</li><li>LLM 生成的新描述将涵盖不同的概念和问题，并使用种子程序的库进行实现。</li></ul><h5 id="步骤-3：RAG-生成新的代码"><a href="#步骤-3：RAG-生成新的代码" class="headerlink" title="步骤 3：RAG 生成新的代码"></a>步骤 3：RAG 生成新的代码</h5><ul><li>使用 RAG，根据新的语言描述生成对应的 Python 代码。</li><li>RAG 管线：<ol><li>从种子程序中检索与新描述相似的问题，获取相关代码。</li><li>根据检索到的代码和新的语言描述，使用 LLM 生成新代码。</li></ol></li></ul><h5 id="步骤-4：执行和过滤"><a href="#步骤-4：执行和过滤" class="headerlink" title="步骤 4：执行和过滤"></a>步骤 4：执行和过滤</h5><ul><li>执行生成的代码，生成输入输出数据对。</li><li>对数据进行过滤，确保其符合要求：<ul><li>可执行性： 代码能够正常运行并生成输入输出数据对。</li><li>确定性： 代码执行结果与随机种子一致。</li><li>输入输出网格大小： 适应 ARC-AGI 的网格大小限制。</li><li>颜色置换不变性： 代码对颜色置换保持不变。</li><li>非平凡问题： 代码生成的输入输出数据对不是恒等变换。</li></ul></li></ul><h5 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h5><ul><li>数据生成管线能够生成大量的合成问题对，每个问题对都包含 Python 解决方案。</li><li>这些合成问题对可以用于训练归纳模型和转导模型，并提高 few-shot 函数学习模型的性能。</li></ul><p>总之通过这套数据生成管线，他们能从100个种子开发出400k个训练数据</p><h2 id="更多想法"><a href="#更多想法" class="headerlink" title="更多想法"></a>更多想法</h2><h3 id="一段话总结"><a href="#一段话总结" class="headerlink" title="一段话总结"></a>一段话总结</h3><p>这篇论文提出了一种结合归纳和转导学习策略的数据生成管线，用于解决 few-shot learning 中的抽象推理任务，其创新点在于通过合成大量训练数据来增强模型的学习能力，并采用了一种回退机制，首先尝试归纳学习，若失败则转而使用转导学习，从而有效提高了模型在有限样本情况下的泛化能力和预测准确性。</p><h3 id="如何结合（强调一遍）"><a href="#如何结合（强调一遍）" class="headerlink" title="如何结合（强调一遍）"></a>如何结合（强调一遍）</h3><p>在这篇论文中，归纳和转导学习的链接是通过以下步骤实现的：</p><ol><li><strong>归纳学习尝试</strong>：首先，系统使用归纳学习模型来处理训练样本。归纳模型试图从这些样本中学习一个通用的表示或规律，然后使用这个学习到的表示来预测或解释新的测试样本。</li><li><strong>解决方案验证</strong>：在归纳学习过程中，系统会检查模型是否能够找到一个合理的解决方案，即是否能够生成一个假设来准确解释训练样本。如果归纳模型成功找到一个合理的解决方案，那么这个解决方案将被用来处理测试样本。</li><li><strong>转导学习回退</strong>：如果归纳模型无法找到一个合理的解决方案，或者其解决方案无法通过验证，系统将回退到转导学习。转导学习模型不试图学习一个通用的表示，而是直接基于测试样本和训练样本之间的特定关系来预测测试样本的输出。</li><li><strong>集成和决策</strong>：系统将归纳和转导模型的输出进行集成，以做出最终的预测。这种集成可以是简单的选择机制（例如，如果归纳模型失败，则使用转导模型的输出），也可以是更复杂的融合策略，比如对两个模型的预测进行加权平均。<br>总结来说，归纳和转导学习的链接是通过一个分阶段的处理流程实现的，其中归纳学习是首选方法，而转导学习作为后备选项，确保了在归纳学习不成功时仍然能够提供预测。这种链接机制充分利用了两种学习策略的优势，提高了系统在处理 few-shot learning 任务时的鲁棒性和准确性。</li></ol><h3 id="上面没提到的东西"><a href="#上面没提到的东西" class="headerlink" title="上面没提到的东西"></a>上面没提到的东西</h3><p>文章的重点在我看来就是这一套数据生成管线，但是显然这篇论文不只是讲了这一个东西，而是提出了一个完整的系统，也就是BARC系统<br>BARC 系统使用了数据生成管线来生成训练数据，但它不仅仅是一个数据生成管线。<br>BARC 系统包括以下部分：</p><ul><li>数据生成管线： 用于生成大量的合成问题对，每个问题对都包含 Python 解决方案。</li><li>归纳模型： 使用数据生成管线生成的合成数据训练的模型，用于学习潜在函数来解释训练数据。</li><li>转导模型： 使用数据生成管线生成的合成数据训练的模型，用于直接预测测试数据的输出。</li><li>测试时间训练 (TTT)： 一种在测试时间更新模型参数的方法，可以提高转导模型的性能。</li><li>重排序： 一种在测试时间对候选输出进行排序的方法，可以进一步提高转导模型的性能。</li><li>集成模型： 将归纳模型和转导模型结合起来，形成 BARC 系统的核心。<br>而最重要的数据生成管线在 BARC 系统中的作用：</li><li>生成训练数据： 为归纳模型和转导模型提供大量的合成数据。</li><li>提高数据质量： 确保合成问题的质量和多样性，从而提高模型的泛化能力。<br>所以BARC 系统是一个更复杂的系统，它不仅仅是一个数据生成管线，还包括了归纳模型、转导模型、测试时间训练、重排序和集成模型等组成部分。数据生成管线是 BARC 系统的核心，它为 BARC 系统提供了训练数据，并推动了 BARC 系统的性能提升。</li></ul><h3 id="一些闲话"><a href="#一些闲话" class="headerlink" title="一些闲话"></a>一些闲话</h3><p>我正在想，一篇论文显然不会全都是干活的，会有很多注水的篇章，所以我大概率就不会提了，以这篇文章为例，显然把BARC模型和人比点数，比较谁更适合解决什么类型的问题的篇幅对我来说就没啥意思，所以我完全不会提。<br>还有，这篇文章有40多页，我拿到的时候思考了一下为啥这么多，结果一看，居然把代码贴在后面了，这真的有必要吗，我感觉这个给一个开源的repo不久好了？虽然我得说这种带着代码说明的也挺好的，方便大家对着看（</p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>论文其实还讨论了一些类似于归纳和转导到底是不是强互补的问题，这个感觉也挺有意思的，同时我还注意到，这篇文章得出了和前人论文相悖的结论</p><blockquote><p>A study ﬁnding that neural models for induction and transduction are strongly complementary, even when trained on the same problems. This contradicts seminal neural program synthesis work (Devlin et al. (2017), which found induction superior), and contradicts the ﬁndings of the leading ARC team (Cole et al. (2024), which advocates transduction with test-time training).</p></blockquote><p>我觉得把这个能列出来还是一个很自信的行为，这说明作者对自己的结果很相信，我感觉挺有意思的。<br>然后这次还碰到了一个看到好多次但是一直没咋了解的玩意：元学习，感觉也得去了解了解了。。。</p>]]></content>
    
    
    
    <tags>
      
      <tag>TTT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Surprising Effectiveness of Test-Time Training for Abstract Reasoning 论文复现报告（一）：论文通读部分</title>
    <link href="/2024/12/02/The-Surprising-Effectiveness-of-Test-Time-Training-for-Abstract-Reasoning-%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%8A%A5%E5%91%8A%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E8%AE%BA%E6%96%87%E9%80%9A%E8%AF%BB%E9%83%A8%E5%88%86/"/>
    <url>/2024/12/02/The-Surprising-Effectiveness-of-Test-Time-Training-for-Abstract-Reasoning-%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%8A%A5%E5%91%8A%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E8%AE%BA%E6%96%87%E9%80%9A%E8%AF%BB%E9%83%A8%E5%88%86/</url>
    
    <content type="html"><![CDATA[<p>​<br>一，前言<br>复现一篇论文肯定是要先把这个论文理解到位，先贴一下论文的链接：<a href="https://ekinakyurek.github.io/papers/ttt.pdf">https://ekinakyurek.github.io/papers/ttt.pdf</a></p><p>然后这是我第一次复现论文，我也不知道报告要怎么写，但是因为我白天实习上班，所以只能看论文写写博客，这样的话就会写的很细，因为我有很多时间写这个（</p><p>二，正文<br>摘要部分<br>首先是看论文的摘要部分：</p><p>简单来说，就是说语言模型在很多训练集相关的问题上有令人惊艳的发挥，但是在需要复杂推理的新问题上面就表现的很挣扎。所以本文就调察了使用TTT方法——在ARC数据集作为基准的情况下——能在提升模型的推理能力方面在有多有效。然后介绍了一下TTT：在推理期间使用从输入数据导出的损失临时更新模型参数的一种方法，根据输入数据动态调参。然后作者做完实验之后，发现了让TTT发挥作用的三个关键组成部分：(1) 在相似任务上的初始微调 (2) 辅助任务格式和增强 (3) 对每个实例的训练。然后作者说了一下实验结果，显然，sota啦！</p><p>背景介绍部分<br>首先还是先强调了一下，现在的大语言模型在处理已知问题上的才能，但是在处理不在数据集里的问题方面，表现的不尽如人意，然后放了张图表示用了TTT之后，处理不在数据集的问题的能力就提升了不少，能力upupup！</p><p>然后又是提了一些之前提出的解决这个问题的方法，例如：chain-of-thought prompting（通过提供一系列逐步推理的过程来指导模型解决问题，这些推理步骤模仿人类解决问题时的思考链条。通过这样的引导，模型能够更清晰地理解问题结构，更好地捕捉到问题解决的关键路径，从而提高了解决复杂问题的能力）等等。</p><p>由此引出了TTT结构，然后再次介绍了一下TTT和相关的应用–视觉模型和时序模型都能用，行文非常有逻辑。</p><p>此时作者也没再重复这些背景介绍了，直接贴出来了本文的贡献有哪些：</p><p>第一点实际上在摘要里面就重点提过了，第二点列了实验结果数据，第三点是说用了TTT之后以前只能通过程序合成来解决的任务可以直接解决了。</p><p>之后作者写了一段挺有意思的话，大意是：这些结果挑战了符号组件对于解决复杂的任务是必要的假设。因为这些结果说明了解决新推理问题的关键因素可能是在测试期间分配适当的计算资源，而与这些资源是通过符号机制还是神经机制部署无关。</p><p>ARC介绍<br>前面说了很多次ARC数据集了其实，所以作者也是专门开了一个小章节来详细介绍一下ARC数据集：</p><p>抽象与推理语料库（ARC）旨在通过其解决视觉谜题的能力来评估语言模型的抽象推理能力。每个谜题，即任务，由2D网格（最大尺寸为30×30）的输入-输出对组成，这些网格包含用多达10种不同颜色绘制的形状或图案，如图所示。每个对中的输出是通过应用一个直观且通用的变换规则或函数 y &#x3D; f(x) 获得的。在实践中，这些变换高度多样且复杂，从简单的概念（如反射和计数）到更复杂的（如应用重力和路径寻找）。”</p><p>In-context Learning介绍<br>语言模型（LMs）在一定规模下的一种能力，即通过输入示例或指令的上下文信息来适应新任务，而无需更新其参数。这种能力被称为“in-context learning”，它使得语言模型能够直接从输入的示例中学习并生成新的输出。主要就是这几点：</p><p>适应新任务的能力：当语言模型达到一定的规模时，它能够通过观察新的输入示例来适应新的任务，而无需重新训练或调整其参数。这种能力使得语言模型能够快速适应新的语言环境和任务要求。</p><p>上下文信息的使用：在in-context learning中，语言模型会使用一系列的输入-输出对（例如，(x1, y1), …, (xn, yn)）以及一个新的输入xn+1，来生成新的输出yˆn+1。这表明语言模型能够通过观察已有的输入和输出对，以及新的输入，来推断出新的输出。</p><p>in-context learning的特点：in-context learning与传统的机器学习算法不同，它不遵循标准的机器学习框架。它是一种通过上下文信息进行学习的方法，而不是通过数据集的训练来学习。</p><p>局限性：尽管in-context learning在某些情况下表现出色，但它并不是对所有新任务都有效。例如，对于较小的语言模型（例如，只有几亿参数），它们在ARC（抽象与推理语料库）上的表现并不理想。ARC是一个专门用于评估语言模型抽象推理能力的数据集，较小的语言模型在这个数据集上的表现较差，这表明in-context learning可能不适用于所有类型的任务，尤其是那些需要复杂推理能力的任务。</p><p>TTT介绍<br>测试时训练（TTT）允许参数化模型通过动态参数更新在推理过程中进行适应，这种技术是一种归纳学习的形式，其中模型利用测试数据结构来提高其预测。TTT的一般过程如下：从初始模型参数θ0开始，对于每个测试输入（或输入批次），我们首先从测试输入生成训练数据DTTT(dinput)。然后，我们优化这些参数以最小化损失函数L(DTTT; θ)，产生暂时更新的参数θd用于预测。在生成预测后，模型被恢复到原始参数θ0，以处理下一个实例或批次。因此，TTT为每个测试输入训练了一个专门的预测模型，该模型是通过在由该测试输入生成的测试时数据集上微调基础模型获得的。</p><p>在过去的工作中（作者在这里举了一些例子），DTTT通常是通过仅对输入x应用无监督目标（例如，掩蔽自动编码）来构建的。然而，我们考虑的上下文学习（也就是上面介绍的In-context Learning）设置提供了以演示对（x1, y1），…，（xK, yK）形式更丰富的上下文。在这里，应用测试时调整涉及首先构建一个初始语言模型LM，将每个测试输入x映射到一个特定的输入-特定数据集DTTT，然后微调LM以优化某些损失函数L，根据：∑d∈DTTT L(LM(d))，并在数据集上进行优化。最后，从更新的模型中采样以获得最终的预测。</p><p>实验设置<br>这个很重要，作者首先明确了要用控制变量法（To investigate the impact of each TTT component, we conduct experiments by varying one component while holding the others constant at their optimal values (described in their respective sections).）每个部分单独测效果，</p><p>然后说了一下模型配置：8B  Llama-3 models, and 1B, 3B Llama-3.2 models，对显存的要求较高，还有数据集ARC，一次80个问题，作者用的是A100</p><p>TTT过程中的数据集和损失<br>数据生成<br>步骤<br>简单来说，通过一个两步过程来获取DTTT（也就是所谓的对于TTT生成的数据）：首先，我们从给定的训练输入-输出对中创建一组留一在上下文学习任务。然后，我们使用可逆的基于规则的变换对这个集合进行增强。这个过程如下图所示。</p><p>详细的步骤：</p><p>步骤1 - 留一法：通过排除第j个示例对作为测试集，我们可以生成n个不同的任务，每个任务包含n - 1个训练集。我们进一步重新排列训练示例的顺序生成了dj的两个随机排列版本。</p><p>步骤2 - 基于规则的变换：考虑一个可逆的变换t，使得t−1(t(x)) &#x3D; x。对于步骤1中获得的每个任务，使用t生成一个新的增强任务t(dICL)，其中t应用于任务中的每个单独网格。 我们选择简单的变换，这些变换在保留基本关系的同时引入了控制变化，例如旋转、翻转、颜色置换、示例置换、大小缩放等。这是很常见的数据增强的办法。</p><p>这个过程的目的是通过创建合成任务和应用数据增强来丰富测试时训练数据集DTTT，从而提高模型的泛化能力和鲁棒性。</p><p>baseline：端到端学习任务<br>作者为了与上文所述的“测试时间上下文学习”方法进行比较，还评估了“测试时间端到端学习”方法。这种方法直接从示例演示中创建一个监督数据集，将每个输入-输出对视为一个独立的训练实例。不像上下文学习设置，在预测时不使用上下文。</p><p>优化目标<br>作者先声明了他们使用了lora网络让模型实现TTT过程，这样有着更好的计算适应性（这个我之前在玩sd画画的时候训练过几个，不用很大的显存也能训练），然后贴出来了训练目标：</p><p>总之是要最小化标准语言模型在示例和测试集上的loss</p><p>结果展示<br>作者贴了几张图来展示用了TTT之后的结果：</p><p>可以看出微调之后的模型加上TTT的效果非常好</p><p>TTT后的推理策略<br>增强推理<br>作者首先说了根据最近的研究，扩大测试时计算量可以显著提升语言模型的性能。最常用的技术之一是对多个响应进行采样，然后使用排序器选择最佳响应。然而，尽管采样在有多种可能解决方案的领域（例如代码中的程序）或通向最终答案的多条路径（例如数学）中非常有效，但在直接生成答案时可能会产生负面影响，因为没有办法在确保样本内部一致性的同时直接强制样本之间的多样性。作为推理时间扩展的替代方案，本论文使用了一种增强的推理策略，通过几何变换生成多个预测候选，与贪心解码方案相结合。详细来说就是对于具有训练样例 (x, y)_k 和测试输入 x_test 的特定任务，使用可逆的几何变换来生成该任务的等价变换版本，将其应用于所有训练示例和测试输入，并使用这些变换后的输入运行模型。</p><p>预测集成（投票策略）<br>这个方法是通过一种分层的投票策略来最终决定哪一个预测是最好的。基本上分为两个阶段：</p><ol><li><p>变换内投票：我们把所有预测依据其所对应的变换进行分组。在每一组内，我们挑选出最常出现的三个预测。如果某一组中不够三个不同的预测，我们就想办法补足。例如，我们可以通过看每一行或者每一列的最多数值来补充预测。这意味着在这个阶段，我们不仅依据整体预测来挑选，还从细分的数据特征中获取信息。</p></li><li><p>全局投票：在这个阶段，我们把所有变换组的候选预测集合在一起，再次进行投票。最终，我们选择出现次数排名前二的预测作为最终结果。如果有出现次数相同的情况，我们会优先选择那些在无变换（即原始状态）的情况下的预测。</p></li></ol><p>总的来说，这种方法是为了确保生成的预测既多样化又具有一致性，通过逐步投票筛选出最可能的结果。</p><p>结果<br>这里作者说为了评估上面两个方法的效果，做了消融实验（一种用于评估和理解模型性能贡献的实验方法。在机器学习和深度学习中，研究人员会对模型的不同组成部分进行有针对性的移除或修改，以观察这些变化对模型整体性能的影响。通过这种方式，他们可以识别出哪些组件或特征对模型的性能至关重要，哪些则可以去除或简化而不显著影响性能。）（话说为啥不能叫控制变量法）</p><p>显然这是复现论文的时候需要做的实验：</p><p>1.首先要做的事肯定是确定基线模型的效果,作者称之为Vanilla模型简单来说就是啥tricks都没有用到的方法,直接使用训练好的模型来生成预测结果,这个模型为任务的两个不同排列各生成一个预测结果,这个基线模型的设置作为一个参考点，用来评估作者提出的增强推理和投票策略的好处。通过比较基线模型和其他模型的性能，可以更清楚地看到这些策略带来的改进。(其实就是控制变量)</p><p>2.第二个实验应用了特定的数据变换,来评估只使用这一个方法的效果,换句话说，模型在处理经过旋转、转置或翻转的数据时的表现如何,然后还强调了本实验的目的是单独评估每种变换的效果。,也就意味着每次实验只应用一种变换，而不是多种变换的组合，以此来确定每种变换对模型性能的具体影响。而且作者还专门提了一句基线模型实际上也可以被认为是这个类别的一部分,因为变换是恒等的</p><p>3.第三个实验就是用上了分层投票技术</p><ol start="4"><li>第四个实验是用了扁平投票技术</li></ol><p>5.第五个实验,命名为Oracle,作者说这个模型是最理想的投票模型,只要投票集合里有正确的就会把他选出来,他代表了投票系统的上限!</p><p>这张图展示了这个实验的结果:</p><p>可以发现,特定变换版本（如旋转、转置、翻转等）的单独性能通常较差。这意味着当模型只针对某种特定的数据变换进行训练或测试时，其准确度并不高。 在所有变换中，转置变换的准确度最低。然而，通过投票程序对这些变换的预测结果进行聚合，可以显著提高准确度。这表明，尽管单个变换的性能不佳，但将它们结合起来可以提升整体性能。这表明某些任务在其变换版本上可能更容易解决。这可能是因为变换后的数据提供了不同的视角或特征，有助于模型更好地理解任务。</p><p>然后作者也提到了—这是一个公认的有效策略。 实际上，使用分层程序的时候与Oracle模型相当，这表明分层聚合能够有效地选择正确答案（当正确答案存在时）并且具有很高的准确度。这强调了分层投票方法的有效性，它接近于理论上的最佳性能。</p><p>在TTT之前的微调<br>尽管TTT提升了模型的任务适应力,模型的基本能力还是会影响最终的表现.所以作者就有了一些办法来生成合成的训练数据,以此来提升模型的抽象解释能力,探索任务生成的自动和半自动方法.所以在这一节中主要就是讲如何生成微调数据和分析一下不同的数据来源和模型大小对最后表现的影响.</p><p>准备微调数据<br>作者先提到了Hodel (2024) 提供了一个特定领域的语言（DSL），名为 REARC。其中包括为 DARC 训练数据集中的每个任务实现的数据生成函数 gi 和变换函数 fi，这些函数能够生成遵循相同变换原则的新输入-输出对，使得通过 gi 生成的数据可以由相同的 fi 函数解决，从而保持了任务的一致性和可扩展性。</p><p>所以作者接下来就要用这个方法了:</p><p>a)使用现有的生成器，REARC中的生成器函数gs已经通过生成相同任务的不同实例化，提供了一个有效的数据增强工具。通过多次运行这些代码，并随机将这些新示例分割到训练和测试示例集，从这些训练任务中生成额外的样本。</p><p>b)除了使用REARC,作者还用了其他办法,也就是使用少量样本提示（few-shot prompting）和大型语言模型（LLM，如GPT4和GPT4-o的集合）来生成新的任务。简而言之，就是通过以下步骤来创造多样化的任务：首先，利用现有的生成器函数和少量样本，让语言模型生成新的生成器函数。接着，结合任务描述和生成器函数，通过少量样本提示来共同生成任务描述和新生成器。此外，作者还采用了一种两阶段的方法，先生成任务描述，然后再基于这些描述生成新的生成器。通过这些方法，研究者收集了6426个生成器，并通过下图中的过程提供了这些任务描述的生成细节。</p><p>得到了类似这样的结果:</p><p>c)最后后，作者的合成任务通过多种几何变换得到了增强，这些变换包括基本变换（如旋转、反射、随机位移和尺寸缩放）、图案操作（如随机贴片、平铺和重复）、颜色排列，以及涉及多个基本变换顺序应用的复合变换。这些变换以以下三种方式应用：</p><p>仅输入网格：将输入(x, y)变换为(t(x), y)，其中t表示某种几何变换。<br>仅输出网格：将输入(x, y)变换为(x, t(y))。<br>输入和输出都应用：将输入(x, y)变换为(t(x), t(y))。<br>结果<br>得到数据之后,就能微调了,显然,所以作者要开始列出结果了:</p><p>作者使用了增强数据对1B和3B参数的Llama 3.2指令调整模型以及8B参数的Llama 3指令调整模型进行了全面微调。微调的格式和训练目标与第2节(也就是背景介绍部分)中描述的TTT模型相同。。对于增强数据，作者进行了以下消融实验：</p><ol><li><p>No FT：未进行任何微调的原始Llama 3指令调整模型。</p></li><li><p>All：我们使用了第上面中描述的所有方法，包括REARC、基于规则的增强和语言模型生成。</p></li><li><p>No-Geom：我们从所有任务中移除了几何变换。</p></li><li><p>No-LM：我们仅使用REARC和基于规则的增强，排除了由语言模型生成的任务。</p></li></ol><p>结果如图:</p><p>左边是微调模型在不同数据上的表现,可以看出,加上TTT之后的表现了极大的提升</p><p>右边是不同模型大小之间的表现区别,TTT同样表现了很好的效果</p><p>所以FT的数据是如何影响TTT的呢?</p><p>作者说通过上图,其实可以看出来使用REARC和结合基于规则的增强方法训练的模型在TTT任务上取得了最强的性能。这表明这种组合方法是有效的，可以提高模型的泛化能力。但是ALL部分多了使用LM模型生层数据却导致低了5个点,这表明当前的基于LM的任务生成方法可能存在一些问题，例如生成的任务质量不高或者与实际任务不匹配。最后，研究发现微调阶段的性能与TTT阶段的性能之间几乎没有相关性。这意味着即使模型在微调数据上表现很好，也不一定意味着它在TTT任务上会有很好的表现。这可能是由于TTT任务与微调任务之间的差异导致的。</p><p>那么模型的参数对于结果的影响情况呢?</p><p>随着模型参数量的增加，模型在微调阶段的性能也有所提高。在所提供的数据中，8B（80亿参数）的模型在微调后达到了最高的准确率，即36%。而通过TTT过程，参数量较小的模型（如1B和3B模型）在性能上得以提升，以至于它们在TTT后的准确率与较大模型相似。</p><p>ARC 基准和它与其他系统的比较<br>在这项研究中，作者在80个任务上进行了开发实验后，在完整的ARC公共评估集上展示了全面的结果，并将他们的系统与现有方法进行了比较.比较侧重于三个关键方面：他们TTT（任务到任务转换）方法的影响、将他们的方法与现有方法结合的好处，以及全神经网络方法与程序合成方法之间的差异。结果:</p><p>结论<br>在本研究中，作者探讨了测试时训练（test-time training）的效果，并证明了它能够显著提升语言模型在ARC数据集上的性能。同时还发现了学习特定于任务的LoRA适配器和使用几何变换生成增强的测试时数据集至关重要。此外，作者还开发了一个增强的推理流程，利用可逆变换生成多个预测，并通过自一致性选择最佳候选。我们的整体流程应用了多种测试时计算方法，每个组件都有正向的贡献。这表明TTT不仅能提升LM性能，不同的测试时方法还能相互补充.作者的TTT流程与现有方法（BARC）结合，在ARC公共数据集上实现了最佳结果，并与平均人类表现相当。我们的发现暗示，测试时方法可能在推动下一代LM的发展中扮演关键角色。然而，他们的研究也存在局限性，包括评估框架的限制、实验重现性的问题，以及数据泄露的可能性。 </p><p>三，要做的实验总结<br>显然主要要做的实验就是每一个消融实验!</p><p>第一部分</p><p>这里要做的实验有六个,我先详细讲一下我的思路:</p><p>1.第一个实验,也就是用的没有TTT结构的微调模型来跑,作为baseline,但是有个问题是没有提到这个实验是在哪种类型的模型上实验的(姑且认为是Llama-3 8B)</p><p>2.第二个实验,就是在增强数据里边去掉了第二步的转换生成步骤,得到未经过变换增强的测试时训练数据集,再进行TTT实验</p><p>3.第三个实验,按照论文中描述的端到端任务公式创建数据集,将每个输入 - 输出对视为独立训练实例,不使用上下文信息（与 in - context 学习设置不同）。同样可以对这些数据应用规则变换来增强数据集（类似于其他实验中的变换操作，但数据组织形式为端到端格式</p><p>4.第四个实验,按照常规方式获取任务的训练输入 - 输出对，并进行数据生成管道中的 leave - one - out 任务创建及规则变换增强数据操作，得到完整的测试时训练数据集（包含多个任务的增强数据）。但与常规 TTT 不同的是，在学习 LoRA 适配器时，不是为每个任务学习单独的适配器，而是使用所有任务的聚合数据集学习单个 LoRA 适配器。利用这个共享的 LoRA 适配器，在测试时对模型参数进行优化（使用 AdamW 优化器，训练 2 个 epoch，批大小为 2 等常规设置）</p><p>5.第五个实验,按照常规的测试时训练数据生成方式，包括 leave - one - out 任务创建和规则变换增强数据，得到测试时训练数据集,在优化模型参数时，只计算测试输出的损失，不考虑演示数据（即训练示例中的输入 - 输出对）的损失。其他训练设置（如使用 LoRA 参数更新、AdamW 优化器、训练 epoch 和批大小等）保持不变。</p><p>6.第六个实验,前面都一样,区别在于在训练模型时，不使用全精度的基础模型更新，而是为每个任务学习量化的 LoRA 适配器，以提高内存效率。其他训练设置（如优化器使用 AdamW、训练 2 个 epoch、批大小为 2 等）与常规 TTT 一致。</p><p>第二部分</p><p>这里要做的实验有五个,我先详细讲一下我的思路:</p><p>1.第一个实验,基准模型,使用标准推理方法，不进行任何增强推理操作（如不使用几何变换生成多个预测版本）和投票操作。仅对任务的两种排列，从模型中生成 2 个预测结果，不进行额外的处理或聚合操作。</p><p>2.第二个实验，评估单个几何变换（旋转、转置、翻转等）在推理过程中的单独有效性，了解每个变换对模型预测性能的影响，为后续确定最佳变换组合或策略提供依据。对于每个任务，分别应用特定的几何变换（如仅旋转 90 度、仅进行转置、仅垂直翻转等）到训练示例和测试输入上，生成变换后的任务版本。使用模型对每个变换后的任务版本进行预测，得到相应的预测结果，每个变换独立进行预测，不进行变换间的组合或投票操作。</p><p>3.第三个实验，首先进行增强推理操作，使用可逆几何变换（如旋转、翻转等）和训练示例排列，生成多个预测候选。例如，对于每个变换，应用于训练示例和测试输入后得到变换后的任务，然后从模型中生成预测结果，并通过逆变换得到最终预测，每个任务总共生成个预测（为训练示例排列数，为变换集合）。接着进行分层投票操作，包括两个阶段：第一阶段在每个变换内进行投票，选择最频繁的预测（若少于 3 个独特预测，则通过行 &#x2F; 列多数投票补充）；第二阶段在变换特定候选中进行全局投票，选择最频繁的 2 个预测作为最终输出。</p><p>4.第四个实验，同样先进行增强推理操作，生成个预测候选。然后对所有预测候选进行一次全局投票，直接选择最频繁的 2 个预测作为最终输出，不进行分层投票中的变换内投票和补充候选操作。</p><p>5.第五个实验，在生成个预测候选后，Oracle 直接从这些候选中选择正确答案（如果存在）。这个估计就是遍历一下答案来看。</p><p>第三部分</p><p>这里有四个实验，我说一下我的思路</p><p>1.No FT 实验，直接使用原始的 Llama 3 指令调优模型，不进行任何额外的微调操作。在测试时，按照常规的推理方式，将测试任务输入到模型中，获取模型的预测结果。不涉及训练过程中的参数更新或数据增强等操作。</p><p>2.All 实验，数据准备阶段，综合运用 Section 5.1 中描述的所有方法生成微调数据。</p><p>利用 REARC 中的生成器函数生成额外样本，并进行适当的分割用于训练和测试。<br>通过语言模型（如 GPT - 4 和 GPT - 4o）采用多种方式（如简单 few - shot 示例生成、生成器与任务描述联合生成、两阶段生成）生成新的任务，并收集这些任务数据。<br>对合成任务应用各种几何变换（如旋转、反射、随机平移和缩放等）进行增强，变换方式包括仅应用于输入网格、仅应用于输出网格或同时应用于输入和输出网格，且以 30% 的概率随机应用这些变换。<br>使用上述生成的增强数据对 1B、3B Llama 3.2 指令调优模型和 8B Llama 3 指令调优模型进行完整的微调训练。训练过程中的格式和训练目标与 Section 2.4 中描述的 TTT 过程相同，具体超参数设置参考附录 B.2（例如使用 AdamW 优化器、特定的学习率、训练 epoch 数和批大小等）。</p><p>在微调训练完成后，使用模型对测试任务进行预测，获取预测结果。</p><p>3.No - Geom 实验，数据准备阶段，使用 REARC 和规则增强方法生成微调数据，但不应用任何几何变换。即不进行如旋转、平移、缩放等几何操作来改变任务的输入或输出。仅依靠 REARC 中的生成器函数生成样本和规则增强（如可能的颜色置换、示例置换等非几何变换操作）来构建微调数据集。使用上述数据对模型进行微调训练，训练过程遵循常规的微调设置（如使用 Llama 系列模型、特定的优化器、训练目标和超参数等，同 All 实验中的训练设置，但数据不含几何变换增强）。微调训练完成后，使用模型对测试任务进行预测，得到预测结果。</p><p>4.No - LM 实验，数据准备阶段，仅利用 REARC 中的生成器函数生成样本，并通过规则增强（如颜色置换、示例置换、大小缩放等）来丰富数据，不包含语言模型生成的任务数据。即不使用如 few - shot 示例生成、生成器与任务描述联合生成等语言模型相关的数据生成方式。使用上述准备的数据对模型进行微调训练，训练过程与其他微调实验保持一致（如模型选择、优化器、训练目标和超参数设置等）。微调训练完成后，使用模型对测试任务进行预测，获取预测结果。</p><p>四，后记<br>所以看起来要做的实验还是很多的，目前我也只是把作者提供的代码跑了起来，还没有完全搞明白该怎么调整来进行上述的所有实验，所以，还是慢慢来吧。。。好消息是显卡资源应该是够的，因为所有实验涉及到训练的只有微调,而我现在能用的是一块A100的40g版本，8B的版本也能跑一下lora训练，速度的话，只能说还得再观察一下，祝我进展顺利！</p><p>​</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>

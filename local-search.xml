<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>The Surprising Effectiveness of Test-Time Training for Abstract Reasoning 论文复现报告（二）：实验报告States 论文阅读报告</title>
    <link href="/2024/12/10/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%8A%A5%E5%91%8A%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/"/>
    <url>/2024/12/10/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%8A%A5%E5%91%8A%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>第一部分的实验还是挺多的，也是一个熟悉微调大模型的好机会，总之先开始吧</p><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><h2 id="第一个实验：用无TTT结构的微调模型跑实验"><a href="#第一个实验：用无TTT结构的微调模型跑实验" class="headerlink" title="第一个实验：用无TTT结构的微调模型跑实验"></a>第一个实验：用无TTT结构的微调模型跑实验</h2><h3 id="具体参数："><a href="#具体参数：" class="headerlink" title="具体参数："></a>具体参数：</h3><h4 id="模型：llama3-8B-微调版本"><a href="#模型：llama3-8B-微调版本" class="headerlink" title="模型：llama3-8B(微调版本)"></a>模型：llama3-8B(微调版本)</h4><h4 id="数据：419个问题（来自ARC和增强数据）"><a href="#数据：419个问题（来自ARC和增强数据）" class="headerlink" title="数据：419个问题（来自ARC和增强数据）"></a>数据：419个问题（来自ARC和增强数据）</h4><h4 id="设备：A100-pcie-40gb"><a href="#设备：A100-pcie-40gb" class="headerlink" title="设备：A100-pcie-40gb"></a>设备：A100-pcie-40gb</h4><h4 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h4><ul><li>首先用作者给出的微调模型跑一下实验</li><li>记录实验结果</li></ul><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><ul><li>第一次：<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/bf62f1248203cc8513bf6404c87727b.png"></li><li>第二次:<br><img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/2b2e497301ef27acc42f103f97c5287.png"><br>这里贴一下对于400和419的理解：</li></ul><h4 id="关于-419"><a href="#关于-419" class="headerlink" title="关于 419"></a>关于 <code>419</code></h4><ol><li><p><strong>来源及含义</strong>：<br>从代码中可以看到 <code>args.num_examples</code> 参数的默认值被设置为 <code>419</code>，其代表的含义是用于限制处理的任务数量。也就是通过 <code>read_tasks_from_single_file</code> 函数从指定的数据文件（<code>args.data_file</code>）读取任务后，会对任务列表进行操作，如果 <code>args.num_examples</code> 有值（这里默认就是 <code>419</code>），会截取任务列表的前 <code>419</code> 个任务来参与后续一系列的处理流程，例如预处理、生成输入给模型的查询、进行预测以及后续的评估等操作。所以这个 <code>419</code> 体现的是最初设定要参与处理的任务数量上限（当然，如果实际数据文件里任务总数不足 <code>419</code> 个，那就是实际的任务数量）。<br><img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/20241209225848643.png"></p></li><li><p><strong>在准确率计算中的体现</strong>：<br>在 “<code>Per Prediction Accuracy: 42 / 419 = 0.10023866348448687</code>” 这一计算中，分母使用 <code>419</code> 就是基于上述设定。表示在所有设定要参与处理的这 <code>419</code> 个任务里，统计预测准确的任务数量占总任务数量（也就是 <code>419</code> 个）的比例，以此来衡量预测准确率情况。</p></li></ol><h4 id="关于-400"><a href="#关于-400" class="headerlink" title="关于 400"></a>关于 <code>400</code></h4><ol><li><p><strong>来源及含义</strong>：<br><code>400</code> 这个数字出现在 “<code>Attempted tasks: 400</code>” 和 “<code>Competition Accuracy: 40 / 400 = 0.1</code>” 中。结合代码逻辑来看，经过前面一系列的任务预处理、筛选（例如根据任务是否有效等条件筛选，像在 <code>get_preprocessed_tasks</code> 函数执行后区分出 <code>valid_tasks</code> 和 <code>invalid_tasks</code> ）、生成输入给模型的查询等操作后，最终实际真正去进行后续核心处理（比如将输入给到模型进行预测等相关操作）的任务数量是 <code>400</code> 个。也就是说，虽然一开始设定要处理 <code>419</code> 个任务，但由于部分任务可能因为不符合某些条件（比如预处理后无效、相关查询生成不符合要求等原因）被排除掉了，剩下 <code>400</code> 个任务进入到后续进一步的预测、评估等环节，这里的 <code>400</code> 体现的是实际参与后续关键处理流程的有效任务数量。<br><img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/20241209230404207.png"></p></li><li><p><strong>在准确率计算中的体现</strong>：<br>在 “<code>Competition Accuracy: 40 / 400 = 0.1</code>” 这一计算中，分母使用 <code>400</code> 就是基于上述实际参与后续关键流程的任务数量情况。即统计在这 <code>400</code> 个实际尝试进行处理（如预测等操作）的任务里，达到竞争准确率评判标准（具体评判标准应该和竞赛相关要求有关，代码中通过 <code>evaluate</code> 等函数实现相应评估逻辑）的任务数量占 <code>400</code> 个任务的比例，以此来衡量竞争准确率情况。</p></li></ol><p>综上所述，<code>419</code> 是最初设定参与处理的任务数量上限，而 <code>400</code> 则是经过前面多轮筛选、处理后实际参与后续关键流程的任务数量，它们在不同的准确率等评估指标计算中分别作为分母，来体现对应不同阶段和不同评判标准下的任务相关统计情况。 </p><h2 id="第二个实验-用TTT结构正常跑一次-8B-3B-1B"><a href="#第二个实验-用TTT结构正常跑一次-8B-3B-1B" class="headerlink" title="第二个实验:用TTT结构正常跑一次(8B,3B,1B)"></a>第二个实验:用TTT结构正常跑一次(8B,3B,1B)</h2><h3 id="具体参数：-1"><a href="#具体参数：-1" class="headerlink" title="具体参数："></a>具体参数：</h3><h4 id="模型：llama3-8B-微调版本-llama3-2-1B-llama3-2-3B"><a href="#模型：llama3-8B-微调版本-llama3-2-1B-llama3-2-3B" class="headerlink" title="模型：llama3-8B(微调版本),llama3.2-1B,llama3.2-3B"></a>模型：llama3-8B(微调版本),llama3.2-1B,llama3.2-3B</h4><h4 id="数据：419个问题（来自ARC和增强数据）-1"><a href="#数据：419个问题（来自ARC和增强数据）-1" class="headerlink" title="数据：419个问题（来自ARC和增强数据）"></a>数据：419个问题（来自ARC和增强数据）</h4><h4 id="设备：A100-pcie-40gb-1"><a href="#设备：A100-pcie-40gb-1" class="headerlink" title="设备：A100-pcie-40gb"></a>设备：A100-pcie-40gb</h4><h4 id="训练轮数-2"><a href="#训练轮数-2" class="headerlink" title="训练轮数:2"></a>训练轮数:2</h4><h4 id="batch-size-1-8B-2-3B-3-1B"><a href="#batch-size-1-8B-2-3B-3-1B" class="headerlink" title="batch_size:1(8B),2(3B),3(1B)"></a>batch_size:1(8B),2(3B),3(1B)</h4><h4 id="操作步骤-1"><a href="#操作步骤-1" class="headerlink" title="操作步骤"></a>操作步骤</h4><ul><li>首先训练</li><li>然后逐个推理</li><li>记录实验结果</li></ul><h3 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a>实验结果</h3><ol><li>8B:<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/f05cb2b6acfb4ec3b19b90cb7d9e0d9.png"></li><li>3B:<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/0a3432f02bc49d542264d4b177ce874.png"></li><li>1B:<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/bf918520bf71a9db74c5eb1186a5fe8.png"></li></ol><p>这里实际上看得出来随着模型的参数变大,准确率也在上升</p><h2 id="第三个实验-使用标准推理方法，不进行任何增强推理操作"><a href="#第三个实验-使用标准推理方法，不进行任何增强推理操作" class="headerlink" title="第三个实验:使用标准推理方法，不进行任何增强推理操作"></a>第三个实验:使用标准推理方法，不进行任何增强推理操作</h2><h3 id="具体参数：-2"><a href="#具体参数：-2" class="headerlink" title="具体参数："></a>具体参数：</h3><h4 id="模型：llama3-8B-微调版本-1"><a href="#模型：llama3-8B-微调版本-1" class="headerlink" title="模型：llama3-8B(微调版本)"></a>模型：llama3-8B(微调版本)</h4><h4 id="数据：419个问题（来自ARC和增强数据）-2"><a href="#数据：419个问题（来自ARC和增强数据）-2" class="headerlink" title="数据：419个问题（来自ARC和增强数据）"></a>数据：419个问题（来自ARC和增强数据）</h4><h4 id="设备：A100-pcie-40gb-2"><a href="#设备：A100-pcie-40gb-2" class="headerlink" title="设备：A100-pcie-40gb"></a>设备：A100-pcie-40gb</h4><h3 id="操作步骤-2"><a href="#操作步骤-2" class="headerlink" title="操作步骤:"></a>操作步骤:</h3><ol><li>修改代码,主要修改部分:<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/20241209232139617.png">改成:<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/20241209232254962.png">显然,主要是把这几个数据增强手段给删了</li><li>针对修改写新的脚本,执行即可</li></ol><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><ol><li>第一次:</li></ol><p><img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/7171a9bb7dfc58647553699c0d07488.png"></p><p>2.第二次:<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/20241210113229744.png"></p><p>可以看出来,掉了一些点</p><h2 id="第四个实验-评估单个几何变换（旋转、转置、翻转等）在推理过程中的单独有效性"><a href="#第四个实验-评估单个几何变换（旋转、转置、翻转等）在推理过程中的单独有效性" class="headerlink" title="第四个实验:评估单个几何变换（旋转、转置、翻转等）在推理过程中的单独有效性"></a>第四个实验:评估单个几何变换（旋转、转置、翻转等）在推理过程中的单独有效性</h2><h3 id="具体参数：-3"><a href="#具体参数：-3" class="headerlink" title="具体参数："></a>具体参数：</h3><h4 id="模型：llama3-8B-微调版本-2"><a href="#模型：llama3-8B-微调版本-2" class="headerlink" title="模型：llama3-8B(微调版本)"></a>模型：llama3-8B(微调版本)</h4><h4 id="数据：419个问题（来自ARC和增强数据）-3"><a href="#数据：419个问题（来自ARC和增强数据）-3" class="headerlink" title="数据：419个问题（来自ARC和增强数据）"></a>数据：419个问题（来自ARC和增强数据）</h4><h4 id="设备：A100-pcie-40gb-3"><a href="#设备：A100-pcie-40gb-3" class="headerlink" title="设备：A100-pcie-40gb"></a>设备：A100-pcie-40gb</h4><h3 id="操作步骤-3"><a href="#操作步骤-3" class="headerlink" title="操作步骤:"></a>操作步骤:</h3><ol><li>修改代码,主要修改部分:<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/20241209232139617.png">改成:<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/20241209232655946.png">,<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/20241209232717215.png">等等,显然,主要是把这几个数据增强手段给进行单独保留</li><li>针对修改写新的脚本,执行即可</li></ol><h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><ol><li>旋转90度:<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/def052e9f23036f05a3f3ae6c3e9712.png"></li><li>tranpose:<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/4c253acb7cfdf66ed0a69ff9e738710.png"></li><li>Flip1:<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/cf2b584fc27ce9ce8528ce75154ad1f.png"></li><li>Flip0:<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/9d83c9c93eef8fe475d60946c88d89e.png"></li></ol><h2 id="第五个实验-不进行分层投票中的变换内投票和补充候选操作"><a href="#第五个实验-不进行分层投票中的变换内投票和补充候选操作" class="headerlink" title="第五个实验:不进行分层投票中的变换内投票和补充候选操作"></a>第五个实验:不进行分层投票中的变换内投票和补充候选操作</h2><h3 id="具体参数：-4"><a href="#具体参数：-4" class="headerlink" title="具体参数："></a>具体参数：</h3><h4 id="模型：llama3-8B-微调版本-llama3-2-1B-llama3-2-3B-1"><a href="#模型：llama3-8B-微调版本-llama3-2-1B-llama3-2-3B-1" class="headerlink" title="模型：llama3-8B(微调版本),llama3.2-1B,llama3.2-3B"></a>模型：llama3-8B(微调版本),llama3.2-1B,llama3.2-3B</h4><h4 id="数据：419个问题（来自ARC和增强数据）-4"><a href="#数据：419个问题（来自ARC和增强数据）-4" class="headerlink" title="数据：419个问题（来自ARC和增强数据）"></a>数据：419个问题（来自ARC和增强数据）</h4><h4 id="设备：A100-pcie-40gb-4"><a href="#设备：A100-pcie-40gb-4" class="headerlink" title="设备：A100-pcie-40gb"></a>设备：A100-pcie-40gb</h4><h3 id="操作步骤-4"><a href="#操作步骤-4" class="headerlink" title="操作步骤:"></a>操作步骤:</h3><ol><li><p>修改代码,主要修改部分：<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/20241210122310052.png">主要是把</p><p>改成<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/20241210122525881.png"></p><p>本质上来说就是不进行分层投票中的变换内投票和补充候选操作。</p></li><li><p>针对修改写新的脚本,执行即可</p></li></ol><h3 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h3><ol><li>8B：</li><li>3B:</li><li>1B:</li></ol><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1>]]></content>
    
    
    
    <tags>
      
      <tag>TTT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning to Learn at Test Time --- RNNs with Expressive Hidden States 论文阅读报告</title>
    <link href="/2024/12/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A-2/"/>
    <url>/2024/12/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A-2/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这篇文章也是和TTT相关的(虽然是rnn),我看完一遍之后的感觉就是–类似于上次看的CV领域的那一篇TTT结构的方法–本质上就是用一个辅助模型来帮助主模型处理一些训练集里没有的东西,同时,辅助模型的训练方式也都是自监督学习,在cv里用图像翻转,在nlp里就当然使用字符串遮盖预测了,总之 ,文章链接:<a href="https://arxiv.org/pdf/2407.04620">https://arxiv.org/pdf/2407.04620</a></p><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><h2 id="作者想要解决的局限-RNN优秀的复杂度但对长文本无法处理"><a href="#作者想要解决的局限-RNN优秀的复杂度但对长文本无法处理" class="headerlink" title="作者想要解决的局限:RNN优秀的复杂度但对长文本无法处理"></a>作者想要解决的局限:RNN优秀的复杂度但对长文本无法处理</h2><p>我先分两个方向讲一下上面提到的复杂度和对长文本的局限:</p><h3 id="计算复杂度方面"><a href="#计算复杂度方面" class="headerlink" title="计算复杂度方面"></a>计算复杂度方面</h3><ol><li><strong>线性复杂度的含义</strong>：<br>计算复杂度衡量的是随着输入规模的变化，算法运算量增长的趋势。对于循环神经网络（RNN）层而言，它具有线性复杂度，这意味着当输入序列的长度（比如文本中词元的数量）不断增加时，其计算量大致是按照和输入长度成正比的关系增长的。例如，如果输入序列长度变为原来的两倍，那么计算量也大约会变为原来的两倍。相较于自注意力机制那种二次方复杂度（输入长度翻倍，计算量变为原来四倍）来说，从计算量随输入规模增长这个角度，RNN在处理较长序列时，不会像自注意力机制那样面临计算量急剧膨胀的问题，在计算资源消耗的控制上相对更有优势。</li></ol><h3 id="长文本语境表现方面"><a href="#长文本语境表现方面" class="headerlink" title="长文本语境表现方面"></a>长文本语境表现方面</h3><ol><li><strong>隐藏状态及其作用</strong>：<br>在RNN中，隐藏状态起着关键作用。它可以看作是对之前输入信息的一种汇总和编码，每一步处理新的输入时，都会基于当前输入和上一时刻的隐藏状态来更新当前时刻的隐藏状态，从而让网络能够“记住”之前的一些信息，以便对后续输入进行更好的处理。比如在处理自然语言文本时，隐藏状态可以捕捉到句子前文的语义、语法等相关特征，来辅助理解后面出现的词和整个句子的意思。</li><li><strong>表达能力的局限</strong>：<br>然而，RNN的隐藏状态存在表达能力的限制。在长文本语境中，随着输入序列越来越长，隐藏状态需要编码和汇总的信息越来越多，但是其自身的结构和特性决定了它难以有效地对这些海量且复杂的信息进行充分表示。比如说，长文本中可能存在远距离的语义关联、复杂的逻辑结构以及多种层次的语法关系等，RNN的隐藏状态很难把这些丰富的信息都准确地涵盖进去，会出现信息丢失或者表示不全面的情况。</li><li><strong>对整体性能的影响</strong>：<br>由于隐藏状态没办法很好地处理长序列中的复杂信息，这就导致了RNN在长文本语境下的整体性能受限。例如在一些自然语言处理任务中，像语言生成、文本分类等，如果文本很长，RNN可能就无法精准地把握全文的关键语义信息，进而生成不符合逻辑的文本内容，或者在分类任务中做出错误的判断。而且随着序列长度进一步增加，这种性能下降的问题会越发明显，和那些在长语境中表现更好的模型（如Transformer等基于自注意力机制的模型，能更好地捕捉长距离依赖关系）相比，RNN在处理长文本相关任务时就显得力不从心了。</li></ol><p>上面这两条反映了现有 RNN 的尴尬现实。一方面，RNN 的主要优势（相对于 Transformer）在于其线性（相对于二次）复杂度。这种渐近优势只有在实践中针对长上下文才能实现，根据图 3，这在 8k 之后。另一方面，一旦上下文足够长，现有的 RNN（如 Mamba）难以真正利用所依赖的额外信息。</p><p>所以作者想要提出一种新的结构来,能够结合RNN的计算复杂度的优势又能解决他的长文本乏力问题：</p><blockquote><p>TTT layers. Motivated by this observation, we design a new class of sequence modeling layers where the hidden state is a model, and the update rule is a step of self-supervised learning. Because the process of updating the hidden state on a test sequence is equivalent to training a model at test time, this new class of layers is called Test-Time Training (TTT) layers. We introduce two simple instantiations within this class: TTT-Linear and TTT-MLP, where the hidden state is a linear model and a two-layer MLP, respectively. TTT layers can be integrated into any network architecture and optimized end-to-end, similar to RNNs layers and self-attention.</p></blockquote><p>作者用这段话说明了他的方法—<em>TTT层</em>,这段话的核心思想就是:”将隐藏状态本身视为一个机器学习模型，并将更新规则视为自监督学习的一步。”这句话怎么理解呢?简单来说，为了在长文本中保持高效和表达力，我们需要一个更好的压缩启发式算法。具体来说，我们需要将数千甚至数百万个词元压缩成一个隐藏状态，该状态能够有效地捕捉其底层结构和关系。然后我们知道使用自监督训练的模型可以捕捉到其训练数据的底层结构和关系,大型语言模型本身就是很好的例子。它们通过自监督的下一个词预测任务进行训练，其权重可以被视为对互联网上现有知识的压缩存储形式。通过查询大型语言模型，我们可以从它们的权重中提取知识。更重要的是，大型语言模型通常表现出对现有知识之间语义联系的深刻理解，从而表达新的推理片段.</p><h2 id="TTT层"><a href="#TTT层" class="headerlink" title="TTT层"></a>TTT层</h2><h3 id="核心思想阐述"><a href="#核心思想阐述" class="headerlink" title="核心思想阐述"></a>核心思想阐述</h3><ul><li>将隐藏状态作为机器学习模型：<br>通常在传统模型里，隐藏状态更多是作为一种对过往输入信息进行汇总、编码的中间表示形式，起到传递信息的作用。但在这个新的思路中，把隐藏状态本身当作一个机器学习模型来看待，意味着它不再仅仅是被动承载信息，而是可以像常规的机器学习模型（如神经网络等）那样具备自主学习、对输入输出进行处理的能力。例如，它能够基于接收到的输入数据进行更复杂、灵活的特征提取和转换，就如同一个独立的小型预测模型在发挥作用。这里再讲的详细一点:<br><img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/20241210155900531.png"></li></ul><ol><li><strong>传统模型中隐藏状态的训练</strong><ul><li>在传统的序列模型（如简单的RNN）中，隐藏状态确实是在训练过程中被更新的。但是，它的更新主要是基于当前输入和上一时刻的隐藏状态，通过一个相对固定的公式来实现。例如，在基本的RNN中，$h_t &#x3D; f(Wx_t + Uh_{t - 1}+b)$，其中$h_t$是当前时刻的隐藏状态，$x_t$是当前输入，$h_{t-1}$是上一时刻的隐藏状态，$W$、$U$是权重矩阵，$b$是偏置项，$f$是激活函数。这个过程更像是一个信息的传递和简单的转换，隐藏状态主要是对过去输入信息的累积表示。</li></ul></li><li><strong>新模型中隐藏状态作为机器学习模型的区别</strong><ul><li><strong>主动学习能力</strong>：<ul><li>在提出的新模型中，将隐藏状态本身视为一个机器学习模型，这意味着它具有更强的主动学习能力。它不再只是按照固定的公式被动地更新，而是像一个完整的机器学习模型一样，可以对输入进行更复杂的特征提取和处理。例如，隐藏状态可能会根据输入序列的不同模式，自主地调整自己的内部结构（类似于模型参数）来更好地适应这些模式。</li></ul></li><li><strong>功能拓展</strong>：<ul><li>传统隐藏状态主要是用于存储和传递信息，为后续的输出层提供一个中间的表示。而新的隐藏状态作为机器学习模型，可以直接对输入序列进行预测、分类等操作。比如，它可以在没有输出层参与的情况下，直接根据当前的输入和自身的状态，对序列中的下一个元素进行预测，这类似于一个独立的小型预测模型。</li></ul></li><li><strong>学习方式的改变</strong>：<ul><li>传统的隐藏状态更新是基于训练数据集中的标签信息（如果是有监督学习）或者基于重建输入（如果是无监督学习，如自编码器）。而新的隐藏状态作为机器学习模型，其更新规则是自监督学习的一步。这意味着它可以利用输入序列自身的结构特点，例如序列中的顺序关系、重复模式等，来自我更新和优化，而不依赖于外部提供的标签或者特定的重建目标。例如，它可以通过预测序列中的下一个元素，然后根据预测结果和实际的下一个元素之间的差异来进行自我更新，就像一个自监督的语言模型一样。</li></ul></li></ul></li></ol><ul><li>更新规则作为自监督学习的一步：<br>自监督学习是一种不需要人工标注大量数据标签，通过利用数据自身的结构特点来构造监督信息进行学习的方式。把隐藏状态的更新规则设定为自监督学习的一步，就是说隐藏状态在每一次更新的过程中，会像在自监督学习场景下那样，依据数据本身内在的关联和规律来进行自我调整、优化参数，以更好地适应输入序列的各种特征，使得隐藏状态能不断学习到更有效的信息表示方式，强化其对整个序列信息的编码能力。</li></ul><h2 id="自监督任务"><a href="#自监督任务" class="headerlink" title="自监督任务"></a>自监督任务</h2><blockquote><p>Our key idea is to use self-supervised learning to compress the historic context<br>x1, . . . , xt<br> into a hidden state<br>st<br>, by making the context an unlabeled dataset and the state a model. Concretely, the hidden state<br>st<br> is now equivalent to<br>Wt<br>, the weights of a model<br>f<br> , which can be a linear model, a small neural network, or anything else. The output rule is simply:<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/20241210161012358.png"></p></blockquote><p>以下是对这段话的详细理解：</p><h3 id="整体核心思路"><a href="#整体核心思路" class="headerlink" title="整体核心思路"></a>整体核心思路</h3><p>这段话阐述的核心想法是运用自监督学习的方式，对历史上下文信息（也就是序列中从 $x_1$ 到 $x_t$ 这些元素所包含的信息）进行处理，将其压缩并整合进一个隐藏状态 $s_t$ 里。这里把历史上下文当作一个无标签的数据集来看待，同时将隐藏状态视为一个模型，通过这样的设定来实现信息的有效整合与表示。</p><h3 id="关于隐藏状态作为模型及权重的理解"><a href="#关于隐藏状态作为模型及权重的理解" class="headerlink" title="关于隐藏状态作为模型及权重的理解"></a>关于隐藏状态作为模型及权重的理解</h3><ol><li><strong>隐藏状态与模型权重的等价关系</strong>：<br>具体来讲，此时的隐藏状态 $s_t$ 等同于 $W_t$，这里的 $W_t$ 是一个模型 $f$ 的权重。这是一种很独特的视角转换，以往隐藏状态更多是作为一种中间的向量表示，存储之前输入信息的汇总情况，但现在它被赋予了等同于模型权重的角色。也就是说，这个隐藏状态不再仅仅是简单承载信息，而是像模型的权重参数一样，能够决定模型（这里的模型就是 $f$ ）对输入进行怎样的变换、处理以及特征提取等操作。这里顺便比较一下传统情况下的区别:</li></ol><table><thead><tr><th>比较维度</th><th>隐藏状态（传统情况）</th><th>权重（传统情况）</th></tr></thead><tbody><tr><td><strong>定义与本质</strong></td><td>是对过往输入信息进行汇总和编码的中间向量表示，随着输入序列的推进不断更新，动态反映不同时刻输入信息的累积情况。</td><td>是模型中需要学习的参数，定义了不同层之间（如输入到隐藏层、隐藏层到输出层等）的转换关系，是相对固定的数值（训练时调整，训练后基本固定）。</td></tr><tr><td><strong>更新机制</strong></td><td>在每个时间步，根据当前输入以及上一时刻的隐藏状态，按照特定的计算规则（如在RNN中依据相应公式）进行更新，主要是为了整合新输入与已有信息。</td><td>通过训练过程，基于训练数据、损失函数，利用优化算法（如梯度下降等）来不断调整其数值，目的是最小化损失，使模型输出更符合预期，训练结束后在测试阶段基本不再改变。</td></tr><tr><td><strong>功能作用</strong></td><td>起到传递历史信息的作用，便于后续时间步利用这些信息进行进一步计算，辅助模型理解整个序列的特征，最终服务于输出结果的生成。</td><td>决定了输入数据经过模型各层时如何进行变换、进行何种程度的特征提取以及如何映射到输出空间等，从根本上控制着模型的计算流程和对输入的处理方式。</td></tr><tr><td><strong>在序列处理中的变化情况</strong></td><td>会随着输入序列的逐个时间步不断变化，时刻反映序列中不同位置输入信息的综合情况，对于长序列来说其变化贯穿整个序列处理过程。</td><td>在训练阶段按优化算法逐步调整，训练结束后面对不同的输入序列（在测试阶段）通常保持稳定，不会因输入序列中的不同元素而动态变化（除非特殊的可动态调整权重的架构，但不是常规情况）。</td></tr><tr><td><strong>与模型输出的关系</strong></td><td>间接影响输出，作为中间环节，先汇总信息后参与到后续输出的计算中，其状态好坏影响最终输出是否能准确反映序列特征。</td><td>直接决定输出，不同的权重取值会使得相同输入经过模型后产生不同的输出结果，权重是控制输出的关键因素之一。</td></tr></tbody></table><ol><li><strong>模型 $f$ 的多样性</strong>：<br>这个模型 $f$ 可以有多种形式，它可以是一个简单的线性模型，例如线性回归那样的形式，只通过线性变换来处理输入；也可以是一个小型的神经网络，像包含几个隐藏层的多层感知机，能够进行更复杂、非线性的特征映射；甚至可以是其他任意符合需求的模型结构。这种灵活性意味着可以根据具体的任务场景、数据特点等选择合适的模型形式来构建隐藏状态，使其具备相应的信息处理能力。</li></ol><h3 id="输出规则"><a href="#输出规则" class="headerlink" title="输出规则"></a>输出规则</h3><p>从直觉上讲，输出的结果只是对xt的预测，该预测是由f使用更新后的权重Wt进行的。更新规则是针对某个自监督损失ℓ进行梯度下降的一步：<img src="https://blogpictureded.oss-cn-shanghai.aliyuncs.com/blog/20241210162920587.png">这里学习率是η.<br>在这里作者说:</p><blockquote><p>From the compression point of view, every heuristic needs to decide which input to remember or forget. Our<br>W<br> remembers inputs that produce large gradients – intuitively, inputs that make<br>W<br> learn a lot.</p></blockquote><p> 怎么理解呢?</p><h4 id="从压缩角度出发的一般性考量"><a href="#从压缩角度出发的一般性考量" class="headerlink" title="从压缩角度出发的一般性考量"></a>从压缩角度出发的一般性考量</h4><p>在处理序列数据并尝试对其进行压缩表示时（就像把一段长的历史上下文信息压缩进一个隐藏状态那样），各种方法或者启发式策略（heuristic）都面临一个关键问题，那就是需要去决定哪些输入信息应该被记住，哪些又该被忘掉。这是因为不可能无限制地把所有输入都完整保留下来，一方面计算资源和存储资源有限，另一方面也没必要全部保留，需要提取最关键、最有价值的信息进行存储，以形成一种有效的压缩表示，便于后续利用这个表示进行序列相关的任务处理，比如预测、分类等。</p><h4 id="关于“W”记住特定输入的机制"><a href="#关于“W”记住特定输入的机制" class="headerlink" title="关于“W”记住特定输入的机制"></a>关于“W”记住特定输入的机制</h4><p>这里提到的“W”（也就是前面所讲的等同于隐藏状态的模型权重）有着独特的信息选择方式。它会记住那些能够产生较大梯度的输入。从直观层面理解，梯度在机器学习中通常与模型的学习过程密切相关，梯度反映了损失函数相对于模型参数（在这里就是“W”）的变化率。</p><p>当某个输入使得“W”产生较大的梯度时，意味着这个输入对于“W”（也就是这个隐藏状态所代表的模型权重）的调整、学习有着重要的推动作用。可以想象成这个输入带来了很强烈的“信号”，促使模型（通过调整“W”）去更好地适应数据、学到更多有用的知识或者模式。所以，“W”会倾向于记住这样的输入，把它们所包含的信息保留下来，整合进自身的表示当中，而对于那些产生梯度较小的输入，相对来说就不太重要，可能就会被“忽略”或者说忘掉，以此实现一种基于学习重要性的信息筛选和压缩机制，让“W”所代表的隐藏状态能够聚焦于那些对模型学习有较大价值的输入信息，进而更好地完成后续的序列处理任务。</p><h3 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h3><p>这个其实很暴力,就是简单的端到端—直接针对下一个词预测的最终目标优化自监督任务。</p><h2 id="内外环的问题"><a href="#内外环的问题" class="headerlink" title="内外环的问题"></a>内外环的问题</h2><blockquote><p>We refer to training the larger network as the outer loop, and training<br>W<br> within each TTT layer as the inner loop. An important diﬀerence between the two nested learning problems is that the inner-loop gradient ∇ℓ is taken w.r.t.<br>W<br> , the parameters of<br>f<br> , while the outer-loop gradient is taken w.r.t the parameters of the rest of the network, which we will denote by<br>θrest<br>. Throughout this paper, outer-loop parameters are always denoted by<br>θ<br> with various subscripts.</p></blockquote><p>以下是对这段话的详细理解：</p><h3 id="内外环学习的概念界定"><a href="#内外环学习的概念界定" class="headerlink" title="内外环学习的概念界定"></a>内外环学习的概念界定</h3><p>涉及到了两个不同层面的训练过程，分别被定义为外环（outer loop）和内环（inner loop）。</p><ul><li><p><strong>外环（outer loop）</strong>：这里把对整个更大规模网络进行训练的过程称作外环。从整体网络架构层面出发，针对包含了多个部分、多个层次以及众多参数的完整网络进行的一种全局的训练操作。这个完整网络中除了我们重点关注的 TTT（测试时训练）层里面的相关元素外，还有其他很多组成部分，它们共同协作来完成各种任务，比如处理输入序列、生成合适的输出等，而外环训练就是要调整这些众多组成部分对应的参数，让整个网络的性能达到最优。</p></li><li><p><strong>内环（inner loop）</strong>：与之相对应，在每个 TTT 层内部对 (W) （前面提到过 (W) 等同于 TTT 层隐藏状态所代表的模型 (f) 的参数）进行训练的这个过程被称为内环。也就是说，在 TTT 层这个相对独立又关键的局部范围内，有它自己专门的训练机制，聚焦于对自身隐藏状态所对应的参数 (W) 进行调整优化，使其能够更好地实现如压缩历史上下文信息、进行有效的序列建模等功能。</p></li></ul><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>二者重要区别体现在计算梯度的对象上，内环梯度是关于 (W) （即 TTT 层里隐藏状态对应的模型 (f) 的参数）计算的，通过分析损失函数 (\ell) 相对于 (W) 的变化来指导调整 (W) 参数，助力模型 (f) 在 TTT 层内更好地完成如更新隐藏状态、编码序列信息等工作；外环梯度则针对网络其余部分用 (\theta_{rest}) 表示的参数来计算，通过分析损失函数相对于这些参数的变化去调整它们，促使整个网络从整体架构层面有效处理输入、输出任务. </p><h1 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h1><p>其实论文还有很多部分是在讲他的并行计算和提升效率的问题,但是好像和我没啥关系,就先这样吧()</p>]]></content>
    
    
    
    <tags>
      
      <tag>TTT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models 论文阅读笔记</title>
    <link href="/2024/12/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/12/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>其实和TTT没啥关系，但是还是看，反正我时间很多（链接：<a href="https://arxiv.org/pdf/2411.19477%EF%BC%89">https://arxiv.org/pdf/2411.19477）</a></p><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><h2 id="个人初步总结"><a href="#个人初步总结" class="headerlink" title="个人初步总结"></a>个人初步总结</h2><p>这篇论文实际上就是提出了一套流程，在没有提出新的模型架构的情况下用新的计算流程让整个系统准确率能更高，但是有一定的要求（解决问题的LLM一定要能给出正确的答案，如果给出正确答案的概率是0也白搭），然后作者从数学原理上证明了他这套系统在一定情况下能把错误率减到0。<br>这个就好比什么呢，LLM是一个纯黑盒的回答机器，他会根据问题输出答案，但是得出正确答案的概率会变，可能从5%到90%都有可能，所以一般人都不会真正去相信它，因为没有办法让这个玩意给出正确答案的概率升级到99%，这样大家就会相信它了。<em><strong>所以作者就给出了一套系统，它让我们在完全不改动这个LLM黑盒本身的回答正确概率的情况下，让这个新的系统用LLM黑盒得出正确率能到99%甚至100%的回答，这样就让大家能完全的相信这玩意。</strong></em><br>我看完就说，感觉是在弓箭时代没去研究改变弓箭的结构让箭得更准，而是发明了弓箭手方阵这样总有射中的箭，但是凡事都有代价，这个系统的代价肯定是会在推理阶段花更多的力气。</p><h2 id="作者的系统"><a href="#作者的系统" class="headerlink" title="作者的系统"></a>作者的系统</h2><p>有一说一，作者的系统其实非常好懂，就是大力出奇迹，生成一堆答案，然后通过一个系统来筛选出一个正确率最高的答案，不过我还是认真写一下流程吧，数学分析也会做的（但是会在下一个小节<br>本质上就是两步：</p><h3 id="N个答案的生成"><a href="#N个答案的生成" class="headerlink" title="N个答案的生成"></a>N个答案的生成</h3><p>用多个LLM来生成，很简单吧，但是这篇文章的作者是用70B和72B的模型做的。。。阿里真是财大气粗啊</p><h3 id="淘汰赛"><a href="#淘汰赛" class="headerlink" title="淘汰赛"></a>淘汰赛</h3><p>这个是重点，怎么在这么多的回答中筛出正确的答案呢</p><blockquote><p>for  K times. The winner of each pair is the one that is favored for more than K&#x2F;2 times; ties are broken arbitrarily. Only the winners will move on to the next round. The ﬁnal-round winner at the end of this tournament will be the ﬁnal output of the algorithm.</p></blockquote><p><img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/9f555ceb8ef6b3f9dc35c4a967a79c4.png"></p><p>上面是原文和图片，下面用我的话讲一遍：<br><strong>目标</strong>： 从多个候选答案中选出最佳答案作为最终输出。<br><strong>过程</strong>：</p><ol><li><strong>随机配对</strong>： 将候选答案随机分成一对一对的。</li><li><strong>多次比较</strong>： 对每一对候选答案进行比较 K 次。每次比较都会选择一个胜者。</li><li><strong>选择胜者</strong>： 比较结束后，获得超过 K&#x2F;2 次胜利的候选答案被视为该轮的胜者。如果出现平局，则随机选择一个胜者。</li><li><strong>下一轮</strong>： 只有胜者才能进入下一轮比赛。</li><li><strong>最终结果</strong>： 经过多轮比赛，最终剩下的胜者即为算法的最终输出。<br><strong>举例说明</strong>：<br>假设有 8 个候选答案，K &#x3D; 2。</li></ol><ul><li><strong>第一轮</strong>： 将 8 个候选答案随机配对成 4 组，每组进行比较 2 次。例如，假设第一组两个候选答案的比较结果分别为 A 胜 B 和 B 胜 A，那么这一组的胜者为 B。</li><li><strong>第二轮</strong>： 将 4 个胜者再次随机配对成 2 组，每组进行比较 2 次。例如，假设第一组的胜者为 B 和 D，比较结果分别为 B 胜 D 和 D 胜 B，那么这一组的胜者为 D。</li><li><strong>第三轮</strong>： 最后 2 个胜者进行比较 2 次，例如，假设胜者为 D 和 E，比较结果分别为 D 胜 E 和 E 胜 D，那么最终的胜者为 D，即算法的最终输出。<br><strong>这个淘汰赛的过程类似于体育比赛中的淘汰赛制，通过不断筛选，最终选出最佳的候选答案</strong>。<br>那么比较这个动作是谁来做呢？显然比较这个动作是由 LLM 来完成的。LLM 被用作一个黑盒模型，负责评估两个候选答案的优劣，并输出比较结果。</li></ul><p>上面其实就是所有流程了，你可能注意到了，这个玩意需要一些限制才能保证最后的正确率能达到很高，这个也是作者后面的数学推理的基础：</p><ol><li>LLM给出答案的正确率不能是0，不然你再怎么比较也不可能保证这个系统的正确率能到100%，如果有一次给出的都是错误答案那你再选也没辙。</li><li>LLM做比较的正确率大于你随机选择（这个动作的正确率是50%）</li></ol><p>在这两个假设的限制下，这个系统的成功率才能到100%（理论上）</p><h2 id="数学分析部分"><a href="#数学分析部分" class="headerlink" title="数学分析部分"></a>数学分析部分</h2><p>数学分析部分其实也很好理解,没有涉及很多很难的东西.<br>首先先介绍一下后面要用的变量<br>N和K,分别代表着生成N个答案和进行K次比较,然后是<br><img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205221228056.png"><br>在这段描述中：</p><ul><li><p><strong>$M_{gen}$</strong> 代表的是在一次大型语言模型（LLM）调用以生成候选解决方案时，其输出结果的概率分布。也就是说，当利用LLM去尝试生成一个有可能解决给定问题的候选方案时，这个生成过程对应的输出所遵循的概率分布情况由 $M_{gen}$ 来表示。</p></li><li><p><strong>$M_{comp}$</strong> 代表的是在利用LLM对一对解决方案进行比较时，其输出结果的概率分布。例如针对两个已经存在的解决方案，使用LLM来判断它们谁更优或者它们之间的相对关系等情况时，LLM此次比较操作输出所遵循的概率分布就用 $M_{comp}$ 来表示。</p></li></ul><p>所以给定一个输入问题X,在算法的第一个阶段,答案是符合 $M_{gen}$ 的分布的:<br><img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205222015209.png"><br>然后在第二阶段,淘汰赛阶段,对于每一个候选者$(y,y^{\prime})$,算法对于$K$个独立比较结果进行采样:<br><img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205222406190.png"><br>这是基本的流程,然后接下来是两条定理,限制了计算的情况:<br> 1.<br> 第一条定律,模型输出正确答案的概率不能为0,必然要大于0<img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205223149670.png"><br> 2.<br> 第二条定律,比较一对正确和错误的解决方案时比随机猜测做得更好也就是说政正确概率大于50%<img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205223237757.png"></p><p> 在这两条规律的限制下开始推导:</p><ul><li>定理一:如果输入问题满足假设 1，则所提出的两阶段算法在超参数 N和 K下的失败概率有如下界限：<img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205224002893.png"><br> 看起来很复杂的公式,不是么,接下来开始推导:</li></ul><ol><li>第一阶段,也就是生成部分,独立的进行了N次采样,所以我们的无正确解的概率是:<img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205224328454.png">显然</li><li>然后是淘汰赛阶段,这个会稍微复杂一点,先考虑一对正确和错误的候选方案,他们被比较了K次,而且每次比较的结果的正确率—下面的公式称之为µ—-都大于随机选择(也就是说大于50%),Xi表示均值为µ的独立伯努利随机变量：所以我们得出,经历了K次比较之后失败的概率公式:<img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205224553554.png"><img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205224619558.png">这里我就先不显然了,因为这里是做了一定的推理的:</li></ol><ul><li>首先对于最早的不等式我做一个解释,Xi的本质含义就是第i个比较是否正确,所以对于Xi的求和的本质就是看K次比较里正确的总次数,显然比较失败意味着正确识别的次数少于一半,也就是:![]<img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205230347395.png">所以失败的概率实际上会等于上面这个式子,但是作者用了一个放缩先写成了小于等于,再进一步的写法就是:<img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205230628280.png">当平均次数小于等于0.5时，就认为比较失败了(符合第二个假设)。</li><li>然后在此引入均值μ,这一步就是不等式两边减了一个μ,为了后面的不等式做准备</li><li>使用 Hoeffding 不等式:<img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205231212620.png">,所以直接套公式就好,然后又因为已知μ&gt;Pcomp&gt;0.5,所以又能进行一次替换,换成最后的形式,最终完成了对比较这对候选解决方案失败概率的推导和界定。</li></ul><ol start="3"><li>最后就是把两个阶段的概率合在一起来看:在控制淘汰赛阶段失败概率时，先基于生成阶段至少有一个正确初始候选方案这一前提。任选一个正确候选方案，关注其在二叉树结构里通向算法最终输出的路径。通过归纳法可证明，路径上每对候选方案用(K)次LLM调用比较时，若有一个输入候选方案正确，其比较失败概率不超<img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205232012985.png">沿此含log2N对比较的路径，用并集界可知整体比较得出正确结果（即算法最终输出正确）的失败概率不超<img src="https://raw.githubusercontent.com/RidenShogunei/picture_bed/main/blog/20241205232053802.png">，以此对算法在该阶段输出正确结果的概率情况进行了分析界定。 最后，对算法两个阶段的失败事件取并集，完成了对定理 1 的证明。</li></ol><p><em><strong>所以说失败概率随着确定测试时间计算量的超参数 N和 K指数衰减至零。</strong></em></p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>看完的感觉就是：</p><blockquote><p>大力出奇迹啊，作者用的还是72B的和70B的模型做的实验（用了阿里自己的通义千问，他这套系统还得好几个LLM（最多的实验有32LLM个并行）并行执行，阿里是真有钱啊<br>但是这种系统，虽然说没看过完全一样的，但是类似思想看过很多，比如说让LLM对一个问题生成多个答案然后通过投票系统啥的筛出最有可能的答案，这个和作者的并行生成多个回答的区别就在于没他快吧（，后面的筛选系统感觉有很多方法（作者这个用LLM来负责比较的系统还得加个LLM选择的正确概率大于随机选择的正确概率（50%）的假设<br>所以作者的创新点在于数学理论论证和并行生成并行筛选系统？而且他这个实验结果想要复现的话估计只有超级大企业能做吧（</p></blockquote><p>感觉大模型还是得跟企业一起搞啊，不然这么猛的算力高校哪里提供的了啊（</p><p>不过在我和老师交流过后,老师的回应也我也记录一下:</p><blockquote><p>这篇文章的定位其实是和大模型训练的 scaling law 一样。大模型的 scaling law 给大模型的训练提供了一个理论（其实是经验公式）指导，就是如何有效调配算法、数据、模型，比如要想提升模型性能，光增大模型 size 还不行，还需要提升数据集大小。<br>这篇文章是类似的，但关注点不在大模型训练上，而是test-time computation，简单来说就是模型参数不变，而是在模型输出&#x2F;解码过程中做文章。这里就有很多方法，但是大部分都是基于多次输出选最好的，可以是 best-of-N，可以是 voting，可以是更复杂的搜索，甚至可以训练一个小模型来指导大模型的输出&#x2F;解码过程（比如 OpenAI-o1，DeepSeek-o1，QwQ 等）。那么这里就有一个问题，是否在test-time computation 中投入更大的算力（比如并行数更大，搜索深度更深，小模型训练得更好）就可以带来大模型效果的提升呢？我们也需要一个 scaling law 来指导。这就是这篇文章想做的事情，通过大规模实验，来看看是否存在 test-time computation的 scaling law。</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>TTT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 论文阅读报告</title>
    <link href="/2024/12/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A-1/"/>
    <url>/2024/12/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A-1/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>因为最近都在看TTT相关的文章，所以继续看，这篇的来源是之前看的论文的（放一下链接：<a href="https://proceedings.mlr.press/v119/sun20b/sun20b.pdf">https://proceedings.mlr.press/v119/sun20b/sun20b.pdf</a> ）</p><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p>刚看完开头，没想到是在CV领域的一篇论文，果然这种结构其实都是可以通用的吗，虽然和llm没关系但是还是继续看，反正我时间很多（大概</p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>作者是怎么想到这个办法的？为什么会想到这个办法，或许这个问题有时候连作者本人都回答不了，但是我还是会很好奇的揣测一下。我在下面尝试推测一下：</p><ol><li>从绪论部分看的话，作者也是想要解决模型的泛化不强问题<blockquote><p>Supervised learning remains notoriously weak at generalization under distribution shifts. Unless training and test data are drawn from the same distribution, even seemingly minor differences turn out to defeat state-of-the-art models (Recht et al., 2018).</p></blockquote></li></ol><p>（ 真是一个永恒的问题），但是从理论上来说，一个模型的泛化能力再强，碰到训练集里从未出现的玩意之后还是难办，所以作者没有继续跟着前人的步伐，试图通过拓扑结构或训练期间可用的测试分布数据来预测训练和测试分布之间的差异，而是另辟蹊径，想到了直接在测试的时候，根据新的数据来更新参数，<br> 2. 但是问题又来了，<strong>这个时候更新，没有标签怎么办？</strong> 作者就用了自监督训练的办法来解决这个问题，所以接下来要找一个办法进行快速的自监督学习。<br> 3. 然后作者找到了一个很简单的办法（将每个输入图像旋转 90 度的倍数并预测其角度的任务 (Gidaris et al., 2018)。）用这个办法能实现自监督训练。如果多个测试样本以批次到达，可以使用整个批次进行测试时训练。如果样本以在线流的形式到达，可以通过保持参数状态来获得进一步的改进。（显然，这两种数据的数据方式也让作者在之后的流程中开发出了两种TTT办法）<br> 4. 但是如果要全参数重新训练调整的话，是不是效率太低了，可以不可以用一个辅助模型，只更新部分参数就好？这样的话，可以极大的提升效率（我能想到例子就是大模型的lora网络）（这个部分是我猜的）<br> 5. 好像整体的思路都跑通了，那具体怎么来操作呢？那就是我下面章节要说的。</p><h2 id="具体流程"><a href="#具体流程" class="headerlink" title="具体流程"></a>具体流程</h2><p>其实我不打算讲太多，只想提几个我认为重要的点，毕竟这篇文章主要还是给我自己看的（<br>就像上面我说的，作者的思路其实很清晰，但是每一步的实现会有一些细节的操作要提一下：</p><ol><li>自监督学习部分其实没有啥要提的，我感觉这个方法真挺有意思的：<blockquote><p>In this paper, we choose the rotation prediction task (Gidaris et al., 2018), which has been demonstrated to be simple and effective at feature learning for convolutional neural networks. The task simply rotates<br>x in the image plane by one of 0, 90, 180 and 270 degrees and have the model predict the angle of rotation as a fourway classiﬁcation problem.</p></blockquote></li></ol><p>可以说符合我对于简单但是好用办法的想象</p><ol start="2"><li>然后是训练部分，这里其实要提一点：作者提到了联合训练和预测时训练，<br> <strong>这里面联合训练 (Joint Training) 是指在训练阶段，主模型和辅助模型同时进行训练，共享部分参数。</strong><br>  <strong>而预测时训练 (Test-Time Training) 则是指在预测阶段，利用单个未标记测试样本进行自监督学习，更新共享的特征提取器参数，然后再进行预测。</strong></li></ol><ul><li>两者之间的关系：<br>  联合训练为预测时训练提供了基础，使其能够利用共享的特征提取器进行快     速学习。<br>  预测时训练则进一步提高了模型在分布偏移情况下的泛化能力，使其能够适应不同的测试分布。</li><li>总结：<br>联合训练和预测时训练是 Test-Time Training 方法的两个重要组成部分，它们相互配合，共同提高了模型的泛化能力。</li></ul><ol start="3"><li>然后是作者还提出了两种TTT模式：TTT 和 TTT-Online ，是两种训练方式</li></ol><ul><li>TTT (Test-Time Training)：标准版本，对于每个测试样本，使用辅助模型对其进行自监督学习，更新共享的特征提取器参数，然后使用更新后的参数进行预测。</li><li>TTT-Online (Online Test-Time Training)：在线版本，对于每个新的测试样本，使用上一次更新后的参数作为初始参数，进行自监督学习，更新共享的特征提取器参数，然后使用更新后的参数进行预测。这种模式允许模型在测试阶段持续学习，并根据新的测试样本不断调整其参数。<br><em><strong>两种模式的区别：</strong></em></li><li>TTT： 每个测试样本都会独立地进行自监督学习和预测，参数更新后的结果不会影响后续测试样本。</li><li>TTT-Online： 每个测试样本的参数更新都会基于上一次更新后的参数，这意味着模型会持续学习并积累经验，从而更好地适应分布偏移。<br><em><strong>适用场景：</strong></em></li><li>TTT： 适用于测试样本是独立同分布的情况，例如在标准的图像分类任务中。</li><li>TTT-Online： 适用于测试样本不是独立同分布的情况，例如在视频帧识别任务中，后续帧可能受到先前帧的影响。</li></ul><h2 id="我学到了什么"><a href="#我学到了什么" class="headerlink" title="我学到了什么"></a>我学到了什么</h2><p>看这篇论文非常快，作者逻辑很清晰，公式很好理解，方法很明晰，当然也可能是因为结果部分占比比较多而我又跳着看这个（因为结果的各类比较对我来说没有什么作用</p><p>总之看完之后，我感觉我的收获就是对TTT理解更多了一些，TTT本质就是在测试阶段想要使用自监督学习来进行部分参数的调整来应对不在训练集的问题，所以在llm中，用的就是lora来当作辅助模型这个部分来更新参数，我感觉这个操作现在在我看来是一个更加顺理成章的操作了。</p><p>同时看完这篇文章之后我还更加理解了一个问题，就是辅助网络要经历两次训练，一次是跟着主模型训练，一次是推测时训练，也就是上面提到的联合训练和预测时训练，这样操作的话不仅是提升了辅助网络的泛化能力，<strong>还因为辅助模型会生成自监督任务</strong>，所以能够为主模型提供额外的监督信号</p><blockquote><p><strong>因为在联合训练中，使用相同的训练数据同时训练主模型和辅助模型。在训练过程中，主模型的损失函数和辅助模型的损失函数会进行加权求和，并用于计算所有参数的梯度</strong></p></blockquote><p>这有助于防止主模型在训练数据上过拟合，从而提高模型在测试数据上的性能。<br><em><strong>但是需要注意，在联合训练过程中，两个模型的训练任务时不一样的，主任务是图像分类，辅助任务是图像旋转预测，这是辅助模型能为主模型提供额外的监督信号的原因。</strong></em></p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>这次的文章结构又变了，我感觉还是会继续变，随着我看的论文越来越多，我记录的东西的深度和广度一定是会变化的，我想记录的，对我来说有用的东西肯定也是会变化的，这很正常。</p>]]></content>
    
    
    
    <tags>
      
      <tag>TTT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>COMBINING INDUCTION AND TRANSDUCTION FOR ABSTRACT REASONING 论文阅读报告</title>
    <link href="/2024/12/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/"/>
    <url>/2024/12/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这是我最近阅读的第二篇TTT相关的论文（论文链接：<a href="https://arxiv.org/abs/2411.02272">https://arxiv.org/abs/2411.02272</a> ），这篇论文提到的方法在上一篇论文里也用到过，所以读一遍，但是有一点不同，我现在认为从头到尾注解式的讲一遍没有自己的思考，不是真正的阅读，所以我这一篇的行文格式会不太一样。我会先自己把论文认真看一遍，然后把自己的想法按照自己的思路写一遍，这样或许会更有收获？尽管这样的话，行文的顺序就和论文不太一样了，总是先试试吧。</p><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><h2 id="看完一遍的初步想法"><a href="#看完一遍的初步想法" class="headerlink" title="看完一遍的初步想法"></a>看完一遍的初步想法</h2><p>首先看标题的时候应该就知道本文主要的点在于研究inductive and transductive这两个方法的优劣和结合，看摘要发现果然，因为它一直在讨论究竟是inductive好呢还是transductive好呢，但是最后，果不其然，还是两个都有比较好。<br>所以关注的重点显然就在于经典的，是什么，为什么，怎么办</p><h3 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h3><p>在介绍部分实际上作者还是比较详细的讲了为啥有这么一个思路—-因为人类可以从少量数据中泛化推理出很多东西，但是机器显然不行，这种数据量少的学习都可以称之为Few-shot learning（这个在后面也会继续说），然后当然还要介绍一下两个重要的概念，<strong>归纳和转导</strong>，在这里引用作者的一段话 ：</p><blockquote><p>Here we study neural methods for induction and transduction, using few-shot learning problems from ARC-AGI as our testbed. Induction means ﬁrst ﬁnding a function f where f (xtrain) ≈ ytrain<br>, and then predicting ytest &#x3D; f (xtest). Transduction instead outputs ytest without explicit construction of an intermediate function f . Intuitively, induction captures the notion that a learner should ﬁrst explain the training data, then use that explanation to make predictions. Inductive learners can perform better by spending more time optimizing or searching for better explanations, using the training examples xtrain, ytrain to score candidate functions. Transduction instead captures the intuition that the training examples themselves should play a direct role in generating new predictions, and that successful prediction need not require an explicit explanation.</p></blockquote><p>这是在说啥呢，直观地说，归纳体现了学习者应该首先解释训练数据，然后使用该解释进行预测的概念。归纳学习者可以通过花费更多时间优化或搜索更好的解释来提高性能，使用训练示例<br>xtrain, ytrain 来对候选函数进行评分。转导则体现了训练示例本身应该在生成新预测中发挥直接作用，并且成功的预测不需要明确的解释。<br>所以这两种形式看起来是不太相关的，但是作者通过生成大量的合成问题来训练用于归纳和转导的神经网络之后发现了，用于归纳和转导的神经模型具有很强的互补性。这是比较令人惊讶的点，也是作者的创新点：<br><em><strong>在实际应用中，可以先使用归纳学习获取一个通用函数，如果模型的效果不理想或时间限制使然，可以启用转导学习作为回退机制，以增强预测能力。这种结合方法既能利用归纳学习的泛化能力，也能通过转导学习实现对特定问题的优化，从而在实践中达到更好的效果。</strong></em></p><h3 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h3><p>感觉我上面其实已经提到这个部分了，作者有这个创新点的原因主要是发现了归纳和转导模型之间的互补性，而且还有一点：虽然任何一对模型通常都会解决一些不同的问题，但通常这可以归因于不同的先验知识、数据或架构。但是作者还发现了，即使控制了先验知识、数据和架构，大多数通过归纳解决的问题都没有通过转导解决，反之亦然。所以这两类模型能解决的问题之前确实有很明显的互补性。正应如此，作者才会想办法结合两类模型，来达到1+1&gt;2 的操作</p><h3 id="怎么办"><a href="#怎么办" class="headerlink" title="怎么办"></a>怎么办</h3><p>就像我上面已经提到的，作者的创新点就在于结合了转导和归纳模型的优点，但是作者实现结合的办法并不是改变模型的结构，而是改变推理的流程（先转导不行就归纳），同时提出了一个有效的数据生成管线达到的，这也是作者提到的自己的贡献所在：</p><blockquote><p>An automated data generation methodology that starts with 100-160 program solutions for ARC training tasks, and expands them to make 400k new problems paired with Python solutions.</p></blockquote><p>那么具体来说怎么操作呢？</p><h4 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h4><h5 id="通过元学习来训练"><a href="#通过元学习来训练" class="headerlink" title="通过元学习来训练"></a>通过元学习来训练</h5><p>在训练归纳和转导模型时，采用了元学习（meta-learning）的策略。元学习本质上是一种学习如何学习的办法，即通过从大量的小样本学习任务中提取经验，以提升在新任务上的学习效率。以下是对如何使用元学习来训练两个类型模型的解释：</p><h6 id="元学习数据集"><a href="#元学习数据集" class="headerlink" title="元学习数据集"></a>元学习数据集</h6><p>假设有一个元学习数据集，它由许多小样本学习任务构成。每个任务都伴随着一个真实的函数 f，确保对于给定的训练数据和测试数据 ，都满足 f(x) &#x3D; y 。</p><p>这样的数据集提供了一种结构化的环境，使得模型可以从许多不同但相关的任务中获取经验。有了这个数据集后，模型可以在不同问题中练习，以提高在新任务上迅速适应的能力。</p><h6 id="归纳模型的训练"><a href="#归纳模型的训练" class="headerlink" title="归纳模型的训练"></a>归纳模型的训练</h6><p>归纳模型通过学习函数映射来进行推理。它们的训练目标是最小化一种损失函数，该损失函数通常基于预测的函数与实际的函数之间的差异。假设我们有一个对模型预测的函数分布的评估方法，那么归纳学习模型会在元学习数据集上通过调整参数  来最小化这种差异，以便学到一个适用于新数据的通用函数。</p><h6 id="转导模型的训练"><a href="#转导模型的训练" class="headerlink" title="转导模型的训练"></a>转导模型的训练</h6><p>转导模型直接优化特定测试样本的预测准确性。因此，其训练的损失函数会基于模型在这些具体样本上的预测误差来定义。通过在元学习数据集上进行训练，转导模型调整参数，以在给定的特定训练和测试样本条件下改进效果。</p><h5 id="测试归纳和转导模型"><a href="#测试归纳和转导模型" class="headerlink" title="测试归纳和转导模型"></a>测试归纳和转导模型</h5><p>测试归纳和转导。在元学习之后，模型会遇到一个测试时的小样本学习任务(xtrain, ytrain, xtest)。转导模型预测其对ytest通过束搜索近似）最可能的输出。归纳模型从B个函数f1 · · · fB的测试时预算中进行采样，这些函数通过(xtrain, ytrain)进行过滤，最后用于预测ytest &#x3D; f (xtest)。将预测的测试输出记为yˆtest</p><h5 id="归纳和转导的结合"><a href="#归纳和转导的结合" class="headerlink" title="归纳和转导的结合"></a>归纳和转导的结合</h5><p>结合归纳和转导。归纳允许将候选假设与训练样本进行比较，所以我们知道何时归纳找到了一个合理的解决方案，但有时它无法找到任何解决方案。转导具有相反的特性：我们无法检查其预测是否与训练样本匹配，但它总是提供一个候选答案。因此，我们首先尝试归纳，然后在没有候选假设能够解释样本的情况下尝试转导，这就是所谓的回退。</p><h4 id="数据生成方法（重要）"><a href="#数据生成方法（重要）" class="headerlink" title="数据生成方法（重要）"></a>数据生成方法（重要）</h4><h5 id="步骤-1：人工编写种子程序"><a href="#步骤-1：人工编写种子程序" class="headerlink" title="步骤 1：人工编写种子程序"></a>步骤 1：人工编写种子程序</h5><ul><li>人工编写少量解决特定任务的 Python 程序（据作者说是100个），这些程序将作为数据生成的起点。</li><li>每个种子程序包含：<ul><li><strong>自然语言描述</strong>： 解释该程序解决的任务类型和目标。</li><li><strong>Python 函数 transform_grid</strong>： 将输入网格转换为输出网格。</li><li><strong>Python 函数 generate_input</strong>： 随机生成输入网格。</li><li><strong>Python 库</strong>： 提供通用的代码功能，例如生成随机图形、检测对称性、提取物体等。</li></ul></li></ul><h5 id="步骤-2：LLM-生成新的语言描述"><a href="#步骤-2：LLM-生成新的语言描述" class="headerlink" title="步骤 2：LLM 生成新的语言描述"></a>步骤 2：LLM 生成新的语言描述</h5><ul><li>使用 LLM，根据种子程序的描述生成新的自然语言描述。</li><li>LLM 生成的新描述将涵盖不同的概念和问题，并使用种子程序的库进行实现。</li></ul><h5 id="步骤-3：RAG-生成新的代码"><a href="#步骤-3：RAG-生成新的代码" class="headerlink" title="步骤 3：RAG 生成新的代码"></a>步骤 3：RAG 生成新的代码</h5><ul><li>使用 RAG，根据新的语言描述生成对应的 Python 代码。</li><li>RAG 管线：<ol><li>从种子程序中检索与新描述相似的问题，获取相关代码。</li><li>根据检索到的代码和新的语言描述，使用 LLM 生成新代码。</li></ol></li></ul><h5 id="步骤-4：执行和过滤"><a href="#步骤-4：执行和过滤" class="headerlink" title="步骤 4：执行和过滤"></a>步骤 4：执行和过滤</h5><ul><li>执行生成的代码，生成输入输出数据对。</li><li>对数据进行过滤，确保其符合要求：<ul><li>可执行性： 代码能够正常运行并生成输入输出数据对。</li><li>确定性： 代码执行结果与随机种子一致。</li><li>输入输出网格大小： 适应 ARC-AGI 的网格大小限制。</li><li>颜色置换不变性： 代码对颜色置换保持不变。</li><li>非平凡问题： 代码生成的输入输出数据对不是恒等变换。</li></ul></li></ul><h5 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h5><ul><li>数据生成管线能够生成大量的合成问题对，每个问题对都包含 Python 解决方案。</li><li>这些合成问题对可以用于训练归纳模型和转导模型，并提高 few-shot 函数学习模型的性能。</li></ul><p>总之通过这套数据生成管线，他们能从100个种子开发出400k个训练数据</p><h2 id="更多想法"><a href="#更多想法" class="headerlink" title="更多想法"></a>更多想法</h2><h3 id="一段话总结"><a href="#一段话总结" class="headerlink" title="一段话总结"></a>一段话总结</h3><p>这篇论文提出了一种结合归纳和转导学习策略的数据生成管线，用于解决 few-shot learning 中的抽象推理任务，其创新点在于通过合成大量训练数据来增强模型的学习能力，并采用了一种回退机制，首先尝试归纳学习，若失败则转而使用转导学习，从而有效提高了模型在有限样本情况下的泛化能力和预测准确性。</p><h3 id="如何结合（强调一遍）"><a href="#如何结合（强调一遍）" class="headerlink" title="如何结合（强调一遍）"></a>如何结合（强调一遍）</h3><p>在这篇论文中，归纳和转导学习的链接是通过以下步骤实现的：</p><ol><li><strong>归纳学习尝试</strong>：首先，系统使用归纳学习模型来处理训练样本。归纳模型试图从这些样本中学习一个通用的表示或规律，然后使用这个学习到的表示来预测或解释新的测试样本。</li><li><strong>解决方案验证</strong>：在归纳学习过程中，系统会检查模型是否能够找到一个合理的解决方案，即是否能够生成一个假设来准确解释训练样本。如果归纳模型成功找到一个合理的解决方案，那么这个解决方案将被用来处理测试样本。</li><li><strong>转导学习回退</strong>：如果归纳模型无法找到一个合理的解决方案，或者其解决方案无法通过验证，系统将回退到转导学习。转导学习模型不试图学习一个通用的表示，而是直接基于测试样本和训练样本之间的特定关系来预测测试样本的输出。</li><li><strong>集成和决策</strong>：系统将归纳和转导模型的输出进行集成，以做出最终的预测。这种集成可以是简单的选择机制（例如，如果归纳模型失败，则使用转导模型的输出），也可以是更复杂的融合策略，比如对两个模型的预测进行加权平均。<br>总结来说，归纳和转导学习的链接是通过一个分阶段的处理流程实现的，其中归纳学习是首选方法，而转导学习作为后备选项，确保了在归纳学习不成功时仍然能够提供预测。这种链接机制充分利用了两种学习策略的优势，提高了系统在处理 few-shot learning 任务时的鲁棒性和准确性。</li></ol><h3 id="上面没提到的东西"><a href="#上面没提到的东西" class="headerlink" title="上面没提到的东西"></a>上面没提到的东西</h3><p>文章的重点在我看来就是这一套数据生成管线，但是显然这篇论文不只是讲了这一个东西，而是提出了一个完整的系统，也就是BARC系统<br>BARC 系统使用了数据生成管线来生成训练数据，但它不仅仅是一个数据生成管线。<br>BARC 系统包括以下部分：</p><ul><li>数据生成管线： 用于生成大量的合成问题对，每个问题对都包含 Python 解决方案。</li><li>归纳模型： 使用数据生成管线生成的合成数据训练的模型，用于学习潜在函数来解释训练数据。</li><li>转导模型： 使用数据生成管线生成的合成数据训练的模型，用于直接预测测试数据的输出。</li><li>测试时间训练 (TTT)： 一种在测试时间更新模型参数的方法，可以提高转导模型的性能。</li><li>重排序： 一种在测试时间对候选输出进行排序的方法，可以进一步提高转导模型的性能。</li><li>集成模型： 将归纳模型和转导模型结合起来，形成 BARC 系统的核心。<br>而最重要的数据生成管线在 BARC 系统中的作用：</li><li>生成训练数据： 为归纳模型和转导模型提供大量的合成数据。</li><li>提高数据质量： 确保合成问题的质量和多样性，从而提高模型的泛化能力。<br>所以BARC 系统是一个更复杂的系统，它不仅仅是一个数据生成管线，还包括了归纳模型、转导模型、测试时间训练、重排序和集成模型等组成部分。数据生成管线是 BARC 系统的核心，它为 BARC 系统提供了训练数据，并推动了 BARC 系统的性能提升。</li></ul><h3 id="一些闲话"><a href="#一些闲话" class="headerlink" title="一些闲话"></a>一些闲话</h3><p>我正在想，一篇论文显然不会全都是干活的，会有很多注水的篇章，所以我大概率就不会提了，以这篇文章为例，显然把BARC模型和人比点数，比较谁更适合解决什么类型的问题的篇幅对我来说就没啥意思，所以我完全不会提。<br>还有，这篇文章有40多页，我拿到的时候思考了一下为啥这么多，结果一看，居然把代码贴在后面了，这真的有必要吗，我感觉这个给一个开源的repo不久好了？虽然我得说这种带着代码说明的也挺好的，方便大家对着看（</p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>论文其实还讨论了一些类似于归纳和转导到底是不是强互补的问题，这个感觉也挺有意思的，同时我还注意到，这篇文章得出了和前人论文相悖的结论</p><blockquote><p>A study ﬁnding that neural models for induction and transduction are strongly complementary, even when trained on the same problems. This contradicts seminal neural program synthesis work (Devlin et al. (2017), which found induction superior), and contradicts the ﬁndings of the leading ARC team (Cole et al. (2024), which advocates transduction with test-time training).</p></blockquote><p>我觉得把这个能列出来还是一个很自信的行为，这说明作者对自己的结果很相信，我感觉挺有意思的。<br>然后这次还碰到了一个看到好多次但是一直没咋了解的玩意：元学习，感觉也得去了解了解了。。。</p>]]></content>
    
    
    
    <tags>
      
      <tag>TTT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The Surprising Effectiveness of Test-Time Training for Abstract Reasoning 论文复现报告（一）：论文通读部分</title>
    <link href="/2024/12/02/The-Surprising-Effectiveness-of-Test-Time-Training-for-Abstract-Reasoning-%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%8A%A5%E5%91%8A%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E8%AE%BA%E6%96%87%E9%80%9A%E8%AF%BB%E9%83%A8%E5%88%86/"/>
    <url>/2024/12/02/The-Surprising-Effectiveness-of-Test-Time-Training-for-Abstract-Reasoning-%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E6%8A%A5%E5%91%8A%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E8%AE%BA%E6%96%87%E9%80%9A%E8%AF%BB%E9%83%A8%E5%88%86/</url>
    
    <content type="html"><![CDATA[<p>​<br>一，前言<br>复现一篇论文肯定是要先把这个论文理解到位，先贴一下论文的链接：<a href="https://ekinakyurek.github.io/papers/ttt.pdf">https://ekinakyurek.github.io/papers/ttt.pdf</a></p><p>然后这是我第一次复现论文，我也不知道报告要怎么写，但是因为我白天实习上班，所以只能看论文写写博客，这样的话就会写的很细，因为我有很多时间写这个（</p><p>二，正文<br>摘要部分<br>首先是看论文的摘要部分：</p><p>简单来说，就是说语言模型在很多训练集相关的问题上有令人惊艳的发挥，但是在需要复杂推理的新问题上面就表现的很挣扎。所以本文就调察了使用TTT方法——在ARC数据集作为基准的情况下——能在提升模型的推理能力方面在有多有效。然后介绍了一下TTT：在推理期间使用从输入数据导出的损失临时更新模型参数的一种方法，根据输入数据动态调参。然后作者做完实验之后，发现了让TTT发挥作用的三个关键组成部分：(1) 在相似任务上的初始微调 (2) 辅助任务格式和增强 (3) 对每个实例的训练。然后作者说了一下实验结果，显然，sota啦！</p><p>背景介绍部分<br>首先还是先强调了一下，现在的大语言模型在处理已知问题上的才能，但是在处理不在数据集里的问题方面，表现的不尽如人意，然后放了张图表示用了TTT之后，处理不在数据集的问题的能力就提升了不少，能力upupup！</p><p>然后又是提了一些之前提出的解决这个问题的方法，例如：chain-of-thought prompting（通过提供一系列逐步推理的过程来指导模型解决问题，这些推理步骤模仿人类解决问题时的思考链条。通过这样的引导，模型能够更清晰地理解问题结构，更好地捕捉到问题解决的关键路径，从而提高了解决复杂问题的能力）等等。</p><p>由此引出了TTT结构，然后再次介绍了一下TTT和相关的应用–视觉模型和时序模型都能用，行文非常有逻辑。</p><p>此时作者也没再重复这些背景介绍了，直接贴出来了本文的贡献有哪些：</p><p>第一点实际上在摘要里面就重点提过了，第二点列了实验结果数据，第三点是说用了TTT之后以前只能通过程序合成来解决的任务可以直接解决了。</p><p>之后作者写了一段挺有意思的话，大意是：这些结果挑战了符号组件对于解决复杂的任务是必要的假设。因为这些结果说明了解决新推理问题的关键因素可能是在测试期间分配适当的计算资源，而与这些资源是通过符号机制还是神经机制部署无关。</p><p>ARC介绍<br>前面说了很多次ARC数据集了其实，所以作者也是专门开了一个小章节来详细介绍一下ARC数据集：</p><p>抽象与推理语料库（ARC）旨在通过其解决视觉谜题的能力来评估语言模型的抽象推理能力。每个谜题，即任务，由2D网格（最大尺寸为30×30）的输入-输出对组成，这些网格包含用多达10种不同颜色绘制的形状或图案，如图所示。每个对中的输出是通过应用一个直观且通用的变换规则或函数 y &#x3D; f(x) 获得的。在实践中，这些变换高度多样且复杂，从简单的概念（如反射和计数）到更复杂的（如应用重力和路径寻找）。”</p><p>In-context Learning介绍<br>语言模型（LMs）在一定规模下的一种能力，即通过输入示例或指令的上下文信息来适应新任务，而无需更新其参数。这种能力被称为“in-context learning”，它使得语言模型能够直接从输入的示例中学习并生成新的输出。主要就是这几点：</p><p>适应新任务的能力：当语言模型达到一定的规模时，它能够通过观察新的输入示例来适应新的任务，而无需重新训练或调整其参数。这种能力使得语言模型能够快速适应新的语言环境和任务要求。</p><p>上下文信息的使用：在in-context learning中，语言模型会使用一系列的输入-输出对（例如，(x1, y1), …, (xn, yn)）以及一个新的输入xn+1，来生成新的输出yˆn+1。这表明语言模型能够通过观察已有的输入和输出对，以及新的输入，来推断出新的输出。</p><p>in-context learning的特点：in-context learning与传统的机器学习算法不同，它不遵循标准的机器学习框架。它是一种通过上下文信息进行学习的方法，而不是通过数据集的训练来学习。</p><p>局限性：尽管in-context learning在某些情况下表现出色，但它并不是对所有新任务都有效。例如，对于较小的语言模型（例如，只有几亿参数），它们在ARC（抽象与推理语料库）上的表现并不理想。ARC是一个专门用于评估语言模型抽象推理能力的数据集，较小的语言模型在这个数据集上的表现较差，这表明in-context learning可能不适用于所有类型的任务，尤其是那些需要复杂推理能力的任务。</p><p>TTT介绍<br>测试时训练（TTT）允许参数化模型通过动态参数更新在推理过程中进行适应，这种技术是一种归纳学习的形式，其中模型利用测试数据结构来提高其预测。TTT的一般过程如下：从初始模型参数θ0开始，对于每个测试输入（或输入批次），我们首先从测试输入生成训练数据DTTT(dinput)。然后，我们优化这些参数以最小化损失函数L(DTTT; θ)，产生暂时更新的参数θd用于预测。在生成预测后，模型被恢复到原始参数θ0，以处理下一个实例或批次。因此，TTT为每个测试输入训练了一个专门的预测模型，该模型是通过在由该测试输入生成的测试时数据集上微调基础模型获得的。</p><p>在过去的工作中（作者在这里举了一些例子），DTTT通常是通过仅对输入x应用无监督目标（例如，掩蔽自动编码）来构建的。然而，我们考虑的上下文学习（也就是上面介绍的In-context Learning）设置提供了以演示对（x1, y1），…，（xK, yK）形式更丰富的上下文。在这里，应用测试时调整涉及首先构建一个初始语言模型LM，将每个测试输入x映射到一个特定的输入-特定数据集DTTT，然后微调LM以优化某些损失函数L，根据：∑d∈DTTT L(LM(d))，并在数据集上进行优化。最后，从更新的模型中采样以获得最终的预测。</p><p>实验设置<br>这个很重要，作者首先明确了要用控制变量法（To investigate the impact of each TTT component, we conduct experiments by varying one component while holding the others constant at their optimal values (described in their respective sections).）每个部分单独测效果，</p><p>然后说了一下模型配置：8B  Llama-3 models, and 1B, 3B Llama-3.2 models，对显存的要求较高，还有数据集ARC，一次80个问题，作者用的是A100</p><p>TTT过程中的数据集和损失<br>数据生成<br>步骤<br>简单来说，通过一个两步过程来获取DTTT（也就是所谓的对于TTT生成的数据）：首先，我们从给定的训练输入-输出对中创建一组留一在上下文学习任务。然后，我们使用可逆的基于规则的变换对这个集合进行增强。这个过程如下图所示。</p><p>详细的步骤：</p><p>步骤1 - 留一法：通过排除第j个示例对作为测试集，我们可以生成n个不同的任务，每个任务包含n - 1个训练集。我们进一步重新排列训练示例的顺序生成了dj的两个随机排列版本。</p><p>步骤2 - 基于规则的变换：考虑一个可逆的变换t，使得t−1(t(x)) &#x3D; x。对于步骤1中获得的每个任务，使用t生成一个新的增强任务t(dICL)，其中t应用于任务中的每个单独网格。 我们选择简单的变换，这些变换在保留基本关系的同时引入了控制变化，例如旋转、翻转、颜色置换、示例置换、大小缩放等。这是很常见的数据增强的办法。</p><p>这个过程的目的是通过创建合成任务和应用数据增强来丰富测试时训练数据集DTTT，从而提高模型的泛化能力和鲁棒性。</p><p>baseline：端到端学习任务<br>作者为了与上文所述的“测试时间上下文学习”方法进行比较，还评估了“测试时间端到端学习”方法。这种方法直接从示例演示中创建一个监督数据集，将每个输入-输出对视为一个独立的训练实例。不像上下文学习设置，在预测时不使用上下文。</p><p>优化目标<br>作者先声明了他们使用了lora网络让模型实现TTT过程，这样有着更好的计算适应性（这个我之前在玩sd画画的时候训练过几个，不用很大的显存也能训练），然后贴出来了训练目标：</p><p>总之是要最小化标准语言模型在示例和测试集上的loss</p><p>结果展示<br>作者贴了几张图来展示用了TTT之后的结果：</p><p>可以看出微调之后的模型加上TTT的效果非常好</p><p>TTT后的推理策略<br>增强推理<br>作者首先说了根据最近的研究，扩大测试时计算量可以显著提升语言模型的性能。最常用的技术之一是对多个响应进行采样，然后使用排序器选择最佳响应。然而，尽管采样在有多种可能解决方案的领域（例如代码中的程序）或通向最终答案的多条路径（例如数学）中非常有效，但在直接生成答案时可能会产生负面影响，因为没有办法在确保样本内部一致性的同时直接强制样本之间的多样性。作为推理时间扩展的替代方案，本论文使用了一种增强的推理策略，通过几何变换生成多个预测候选，与贪心解码方案相结合。详细来说就是对于具有训练样例 (x, y)_k 和测试输入 x_test 的特定任务，使用可逆的几何变换来生成该任务的等价变换版本，将其应用于所有训练示例和测试输入，并使用这些变换后的输入运行模型。</p><p>预测集成（投票策略）<br>这个方法是通过一种分层的投票策略来最终决定哪一个预测是最好的。基本上分为两个阶段：</p><ol><li><p>变换内投票：我们把所有预测依据其所对应的变换进行分组。在每一组内，我们挑选出最常出现的三个预测。如果某一组中不够三个不同的预测，我们就想办法补足。例如，我们可以通过看每一行或者每一列的最多数值来补充预测。这意味着在这个阶段，我们不仅依据整体预测来挑选，还从细分的数据特征中获取信息。</p></li><li><p>全局投票：在这个阶段，我们把所有变换组的候选预测集合在一起，再次进行投票。最终，我们选择出现次数排名前二的预测作为最终结果。如果有出现次数相同的情况，我们会优先选择那些在无变换（即原始状态）的情况下的预测。</p></li></ol><p>总的来说，这种方法是为了确保生成的预测既多样化又具有一致性，通过逐步投票筛选出最可能的结果。</p><p>结果<br>这里作者说为了评估上面两个方法的效果，做了消融实验（一种用于评估和理解模型性能贡献的实验方法。在机器学习和深度学习中，研究人员会对模型的不同组成部分进行有针对性的移除或修改，以观察这些变化对模型整体性能的影响。通过这种方式，他们可以识别出哪些组件或特征对模型的性能至关重要，哪些则可以去除或简化而不显著影响性能。）（话说为啥不能叫控制变量法）</p><p>显然这是复现论文的时候需要做的实验：</p><p>1.首先要做的事肯定是确定基线模型的效果,作者称之为Vanilla模型简单来说就是啥tricks都没有用到的方法,直接使用训练好的模型来生成预测结果,这个模型为任务的两个不同排列各生成一个预测结果,这个基线模型的设置作为一个参考点，用来评估作者提出的增强推理和投票策略的好处。通过比较基线模型和其他模型的性能，可以更清楚地看到这些策略带来的改进。(其实就是控制变量)</p><p>2.第二个实验应用了特定的数据变换,来评估只使用这一个方法的效果,换句话说，模型在处理经过旋转、转置或翻转的数据时的表现如何,然后还强调了本实验的目的是单独评估每种变换的效果。,也就意味着每次实验只应用一种变换，而不是多种变换的组合，以此来确定每种变换对模型性能的具体影响。而且作者还专门提了一句基线模型实际上也可以被认为是这个类别的一部分,因为变换是恒等的</p><p>3.第三个实验就是用上了分层投票技术</p><ol start="4"><li>第四个实验是用了扁平投票技术</li></ol><p>5.第五个实验,命名为Oracle,作者说这个模型是最理想的投票模型,只要投票集合里有正确的就会把他选出来,他代表了投票系统的上限!</p><p>这张图展示了这个实验的结果:</p><p>可以发现,特定变换版本（如旋转、转置、翻转等）的单独性能通常较差。这意味着当模型只针对某种特定的数据变换进行训练或测试时，其准确度并不高。 在所有变换中，转置变换的准确度最低。然而，通过投票程序对这些变换的预测结果进行聚合，可以显著提高准确度。这表明，尽管单个变换的性能不佳，但将它们结合起来可以提升整体性能。这表明某些任务在其变换版本上可能更容易解决。这可能是因为变换后的数据提供了不同的视角或特征，有助于模型更好地理解任务。</p><p>然后作者也提到了—这是一个公认的有效策略。 实际上，使用分层程序的时候与Oracle模型相当，这表明分层聚合能够有效地选择正确答案（当正确答案存在时）并且具有很高的准确度。这强调了分层投票方法的有效性，它接近于理论上的最佳性能。</p><p>在TTT之前的微调<br>尽管TTT提升了模型的任务适应力,模型的基本能力还是会影响最终的表现.所以作者就有了一些办法来生成合成的训练数据,以此来提升模型的抽象解释能力,探索任务生成的自动和半自动方法.所以在这一节中主要就是讲如何生成微调数据和分析一下不同的数据来源和模型大小对最后表现的影响.</p><p>准备微调数据<br>作者先提到了Hodel (2024) 提供了一个特定领域的语言（DSL），名为 REARC。其中包括为 DARC 训练数据集中的每个任务实现的数据生成函数 gi 和变换函数 fi，这些函数能够生成遵循相同变换原则的新输入-输出对，使得通过 gi 生成的数据可以由相同的 fi 函数解决，从而保持了任务的一致性和可扩展性。</p><p>所以作者接下来就要用这个方法了:</p><p>a)使用现有的生成器，REARC中的生成器函数gs已经通过生成相同任务的不同实例化，提供了一个有效的数据增强工具。通过多次运行这些代码，并随机将这些新示例分割到训练和测试示例集，从这些训练任务中生成额外的样本。</p><p>b)除了使用REARC,作者还用了其他办法,也就是使用少量样本提示（few-shot prompting）和大型语言模型（LLM，如GPT4和GPT4-o的集合）来生成新的任务。简而言之，就是通过以下步骤来创造多样化的任务：首先，利用现有的生成器函数和少量样本，让语言模型生成新的生成器函数。接着，结合任务描述和生成器函数，通过少量样本提示来共同生成任务描述和新生成器。此外，作者还采用了一种两阶段的方法，先生成任务描述，然后再基于这些描述生成新的生成器。通过这些方法，研究者收集了6426个生成器，并通过下图中的过程提供了这些任务描述的生成细节。</p><p>得到了类似这样的结果:</p><p>c)最后后，作者的合成任务通过多种几何变换得到了增强，这些变换包括基本变换（如旋转、反射、随机位移和尺寸缩放）、图案操作（如随机贴片、平铺和重复）、颜色排列，以及涉及多个基本变换顺序应用的复合变换。这些变换以以下三种方式应用：</p><p>仅输入网格：将输入(x, y)变换为(t(x), y)，其中t表示某种几何变换。<br>仅输出网格：将输入(x, y)变换为(x, t(y))。<br>输入和输出都应用：将输入(x, y)变换为(t(x), t(y))。<br>结果<br>得到数据之后,就能微调了,显然,所以作者要开始列出结果了:</p><p>作者使用了增强数据对1B和3B参数的Llama 3.2指令调整模型以及8B参数的Llama 3指令调整模型进行了全面微调。微调的格式和训练目标与第2节(也就是背景介绍部分)中描述的TTT模型相同。。对于增强数据，作者进行了以下消融实验：</p><ol><li><p>No FT：未进行任何微调的原始Llama 3指令调整模型。</p></li><li><p>All：我们使用了第上面中描述的所有方法，包括REARC、基于规则的增强和语言模型生成。</p></li><li><p>No-Geom：我们从所有任务中移除了几何变换。</p></li><li><p>No-LM：我们仅使用REARC和基于规则的增强，排除了由语言模型生成的任务。</p></li></ol><p>结果如图:</p><p>左边是微调模型在不同数据上的表现,可以看出,加上TTT之后的表现了极大的提升</p><p>右边是不同模型大小之间的表现区别,TTT同样表现了很好的效果</p><p>所以FT的数据是如何影响TTT的呢?</p><p>作者说通过上图,其实可以看出来使用REARC和结合基于规则的增强方法训练的模型在TTT任务上取得了最强的性能。这表明这种组合方法是有效的，可以提高模型的泛化能力。但是ALL部分多了使用LM模型生层数据却导致低了5个点,这表明当前的基于LM的任务生成方法可能存在一些问题，例如生成的任务质量不高或者与实际任务不匹配。最后，研究发现微调阶段的性能与TTT阶段的性能之间几乎没有相关性。这意味着即使模型在微调数据上表现很好，也不一定意味着它在TTT任务上会有很好的表现。这可能是由于TTT任务与微调任务之间的差异导致的。</p><p>那么模型的参数对于结果的影响情况呢?</p><p>随着模型参数量的增加，模型在微调阶段的性能也有所提高。在所提供的数据中，8B（80亿参数）的模型在微调后达到了最高的准确率，即36%。而通过TTT过程，参数量较小的模型（如1B和3B模型）在性能上得以提升，以至于它们在TTT后的准确率与较大模型相似。</p><p>ARC 基准和它与其他系统的比较<br>在这项研究中，作者在80个任务上进行了开发实验后，在完整的ARC公共评估集上展示了全面的结果，并将他们的系统与现有方法进行了比较.比较侧重于三个关键方面：他们TTT（任务到任务转换）方法的影响、将他们的方法与现有方法结合的好处，以及全神经网络方法与程序合成方法之间的差异。结果:</p><p>结论<br>在本研究中，作者探讨了测试时训练（test-time training）的效果，并证明了它能够显著提升语言模型在ARC数据集上的性能。同时还发现了学习特定于任务的LoRA适配器和使用几何变换生成增强的测试时数据集至关重要。此外，作者还开发了一个增强的推理流程，利用可逆变换生成多个预测，并通过自一致性选择最佳候选。我们的整体流程应用了多种测试时计算方法，每个组件都有正向的贡献。这表明TTT不仅能提升LM性能，不同的测试时方法还能相互补充.作者的TTT流程与现有方法（BARC）结合，在ARC公共数据集上实现了最佳结果，并与平均人类表现相当。我们的发现暗示，测试时方法可能在推动下一代LM的发展中扮演关键角色。然而，他们的研究也存在局限性，包括评估框架的限制、实验重现性的问题，以及数据泄露的可能性。 </p><p>三，要做的实验总结<br>显然主要要做的实验就是每一个消融实验!</p><p>第一部分</p><p>这里要做的实验有六个,我先详细讲一下我的思路:</p><p>1.第一个实验,也就是用的没有TTT结构的微调模型来跑,作为baseline,但是有个问题是没有提到这个实验是在哪种类型的模型上实验的(姑且认为是Llama-3 8B)</p><p>2.第二个实验,就是在增强数据里边去掉了第二步的转换生成步骤,得到未经过变换增强的测试时训练数据集,再进行TTT实验</p><p>3.第三个实验,按照论文中描述的端到端任务公式创建数据集,将每个输入 - 输出对视为独立训练实例,不使用上下文信息（与 in - context 学习设置不同）。同样可以对这些数据应用规则变换来增强数据集（类似于其他实验中的变换操作，但数据组织形式为端到端格式</p><p>4.第四个实验,按照常规方式获取任务的训练输入 - 输出对，并进行数据生成管道中的 leave - one - out 任务创建及规则变换增强数据操作，得到完整的测试时训练数据集（包含多个任务的增强数据）。但与常规 TTT 不同的是，在学习 LoRA 适配器时，不是为每个任务学习单独的适配器，而是使用所有任务的聚合数据集学习单个 LoRA 适配器。利用这个共享的 LoRA 适配器，在测试时对模型参数进行优化（使用 AdamW 优化器，训练 2 个 epoch，批大小为 2 等常规设置）</p><p>5.第五个实验,按照常规的测试时训练数据生成方式，包括 leave - one - out 任务创建和规则变换增强数据，得到测试时训练数据集,在优化模型参数时，只计算测试输出的损失，不考虑演示数据（即训练示例中的输入 - 输出对）的损失。其他训练设置（如使用 LoRA 参数更新、AdamW 优化器、训练 epoch 和批大小等）保持不变。</p><p>6.第六个实验,前面都一样,区别在于在训练模型时，不使用全精度的基础模型更新，而是为每个任务学习量化的 LoRA 适配器，以提高内存效率。其他训练设置（如优化器使用 AdamW、训练 2 个 epoch、批大小为 2 等）与常规 TTT 一致。</p><p>第二部分</p><p>这里要做的实验有五个,我先详细讲一下我的思路:</p><p>1.第一个实验,基准模型,使用标准推理方法，不进行任何增强推理操作（如不使用几何变换生成多个预测版本）和投票操作。仅对任务的两种排列，从模型中生成 2 个预测结果，不进行额外的处理或聚合操作。</p><p>2.第二个实验，评估单个几何变换（旋转、转置、翻转等）在推理过程中的单独有效性，了解每个变换对模型预测性能的影响，为后续确定最佳变换组合或策略提供依据。对于每个任务，分别应用特定的几何变换（如仅旋转 90 度、仅进行转置、仅垂直翻转等）到训练示例和测试输入上，生成变换后的任务版本。使用模型对每个变换后的任务版本进行预测，得到相应的预测结果，每个变换独立进行预测，不进行变换间的组合或投票操作。</p><p>3.第三个实验，首先进行增强推理操作，使用可逆几何变换（如旋转、翻转等）和训练示例排列，生成多个预测候选。例如，对于每个变换，应用于训练示例和测试输入后得到变换后的任务，然后从模型中生成预测结果，并通过逆变换得到最终预测，每个任务总共生成个预测（为训练示例排列数，为变换集合）。接着进行分层投票操作，包括两个阶段：第一阶段在每个变换内进行投票，选择最频繁的预测（若少于 3 个独特预测，则通过行 &#x2F; 列多数投票补充）；第二阶段在变换特定候选中进行全局投票，选择最频繁的 2 个预测作为最终输出。</p><p>4.第四个实验，同样先进行增强推理操作，生成个预测候选。然后对所有预测候选进行一次全局投票，直接选择最频繁的 2 个预测作为最终输出，不进行分层投票中的变换内投票和补充候选操作。</p><p>5.第五个实验，在生成个预测候选后，Oracle 直接从这些候选中选择正确答案（如果存在）。这个估计就是遍历一下答案来看。</p><p>第三部分</p><p>这里有四个实验，我说一下我的思路</p><p>1.No FT 实验，直接使用原始的 Llama 3 指令调优模型，不进行任何额外的微调操作。在测试时，按照常规的推理方式，将测试任务输入到模型中，获取模型的预测结果。不涉及训练过程中的参数更新或数据增强等操作。</p><p>2.All 实验，数据准备阶段，综合运用 Section 5.1 中描述的所有方法生成微调数据。</p><p>利用 REARC 中的生成器函数生成额外样本，并进行适当的分割用于训练和测试。<br>通过语言模型（如 GPT - 4 和 GPT - 4o）采用多种方式（如简单 few - shot 示例生成、生成器与任务描述联合生成、两阶段生成）生成新的任务，并收集这些任务数据。<br>对合成任务应用各种几何变换（如旋转、反射、随机平移和缩放等）进行增强，变换方式包括仅应用于输入网格、仅应用于输出网格或同时应用于输入和输出网格，且以 30% 的概率随机应用这些变换。<br>使用上述生成的增强数据对 1B、3B Llama 3.2 指令调优模型和 8B Llama 3 指令调优模型进行完整的微调训练。训练过程中的格式和训练目标与 Section 2.4 中描述的 TTT 过程相同，具体超参数设置参考附录 B.2（例如使用 AdamW 优化器、特定的学习率、训练 epoch 数和批大小等）。</p><p>在微调训练完成后，使用模型对测试任务进行预测，获取预测结果。</p><p>3.No - Geom 实验，数据准备阶段，使用 REARC 和规则增强方法生成微调数据，但不应用任何几何变换。即不进行如旋转、平移、缩放等几何操作来改变任务的输入或输出。仅依靠 REARC 中的生成器函数生成样本和规则增强（如可能的颜色置换、示例置换等非几何变换操作）来构建微调数据集。使用上述数据对模型进行微调训练，训练过程遵循常规的微调设置（如使用 Llama 系列模型、特定的优化器、训练目标和超参数等，同 All 实验中的训练设置，但数据不含几何变换增强）。微调训练完成后，使用模型对测试任务进行预测，得到预测结果。</p><p>4.No - LM 实验，数据准备阶段，仅利用 REARC 中的生成器函数生成样本，并通过规则增强（如颜色置换、示例置换、大小缩放等）来丰富数据，不包含语言模型生成的任务数据。即不使用如 few - shot 示例生成、生成器与任务描述联合生成等语言模型相关的数据生成方式。使用上述准备的数据对模型进行微调训练，训练过程与其他微调实验保持一致（如模型选择、优化器、训练目标和超参数设置等）。微调训练完成后，使用模型对测试任务进行预测，获取预测结果。</p><p>四，后记<br>所以看起来要做的实验还是很多的，目前我也只是把作者提供的代码跑了起来，还没有完全搞明白该怎么调整来进行上述的所有实验，所以，还是慢慢来吧。。。好消息是显卡资源应该是够的，因为所有实验涉及到训练的只有微调,而我现在能用的是一块A100的40g版本，8B的版本也能跑一下lora训练，速度的话，只能说还得再观察一下，祝我进展顺利！</p><p>​</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
